% Chapter 3 - Methodology
% With Sample Figures From Kevin Hughes

\glsresetall % reset the glossary to expand acronyms again
\chapter[Methodology]{Methodology}\label{ch:Methodology}
\index{Methodology}


This chapter provides a detailed breakdown of the research procedures undertaken to compare the suitability of two deep learning models, \gls{N2N} and \gls{N2V} as potential denoising models for the  LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}. It begins with an in-depth analysis of the user and functional requirements, followed by a detailed analysis of the research design. Theoretical background and analysis of \gls{N2N} and \gls{N2V} models are then addressed, after which an exploration of the various research instruments used to develop them, ranging from software to hardware tools is addressed. Data collection is then explored in detail, exploring the various phantoms used and the rationale behind their use. This is followed by analysing the performance evaluation metrics to evaluate the two models. The chapter culminates with an analysis of the limitations and ethical concerns associated with the study. 

\section{Model Development and Comparison Procedure}
The methodology followed the waterfall approach and is broken down into the following phases and summarised in Figure \ref{fig:method} below:

\begin{enumerate}
    \item {\textbf{Literature Review and Familiarisation:} This phase involved an extensive review of existing medical imaging denoising techniques and learning the tools and technologies essential for the project. This included understanding the operational principles of the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} system, gaining familiarity with \gls{PCD} technology, and working with programming languages commonly used in denoising models, such as Python and MATLAB\textsuperscript{\textregistered}.}

    \item \textbf{Data Collection and Pre-Processing:} This phase involved configuring the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} with the required parameters and scanning the selected phantoms. After scanning the phantoms, the images were exported through DVS\textsuperscript{\textregistered} software for pre-processing, which included normalisation, data type conversion, and the addition of extra dimensions.
    \item \textbf{Model Selection and Design:} This phase involved developing and implementing the suitable models identified in Phase 1, specifically \gls{N2V} and \gls{N2N}, due to their adaptability and ability to train without clean images. Variations of these models were implemented in Python programming language on the Google Colab cloud platform.
    \item \textbf{Model Training and Validation:} The data set was split into a training(80\%) set and a validation(20\%) set. The models were trained in two stages. The first stage involved using ten training images over 50 epochs with 100 training steps per epoch. The second stage involved 29 images over 150 epochs with 100 training steps per epoch. After each phase, the training and validation loss and \gls{MSE} were analysed to evaluate model generalisation, with hyperparameter tuning performed as needed.
    \item \textbf{Model testing and evaluation: }In this phase, a new test set of 10 images not used in training was collected. These images were passed through the models, and performance metrics such as proxy \gls{SNR}, \gls{PSNR}, and \gls{SSIM} were calculated to assess the models' suitability for the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} system.
    \item \textbf{\gls{GUI} Development:} Once the model was tested and the results analysed, a \gls{GUI} was created to enable easy denoising of the images using the two models. This \gls{GUI} is a preliminary step for further integrating the denoising models into the LODOX\textsuperscript{\textregistered} DVS\textsuperscript{\textregistered} Software.
\end{enumerate}

 \begin{figure}[h!]
     \centering
     \includegraphics[width=0.9\linewidth]{3_Chapters//3_Chapter_Methodology//Figures/Waterfall.png}
     \caption{Waterfall models depicting the methodology overview}
     \label{fig:method}
 \end{figure}

 \section{System Requirements} \label{sec:reqs}
 The user requirements for the system are detailed in Table \ref{tab:UR} below:

 \begin{table}[h!] 
    \small
    \centering
    \caption{User Requirements} 
    \label{tab:UR} 
    \begin{tabular}{@{}llp{7cm}@{}} 
    \toprule 
    \textbf{ID }& \textbf{Requirement} &\textbf{Description} \\ 
    \midrule 
    UR-01 & High Image Quality & The model should significantly enhance the clarity and quality of LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} images. \\ 
    UR-02 & Retention of Anatomical Details & The model should preserve critical anatomical structures within the images. \\ 
    UR-03 & Compatibility with Existing Systems & The model should be compatible with existing medical imaging software and systems. \\ 
    UR-04 & Fast Processing Time & The model should process images quickly to support real-time diagnostic workflows. \\ 
    UR-05 & User-Friendly Interface & The model should provide an intuitive interface to apply the denoising model easily. \\ 
    UR-06 & Scalability & The model should handle varying image sizes and resolutions effectively. \\ 
    UR-07 & Robustness Across Noise Levels & The model should effectively reduce noise in images with varying noise intensity levels. \\ 
    UR-08 & Minimal Artifact Introduction & The model should minimise the introduction of artifacts during the denoising process. \\ 
    \bottomrule 
    \end{tabular} 
    \end{table}

    These user requirements are then used to derive the functional requirements of the system which are then  detailed in the Table \ref{tab:FR} below:
    \begin{table}[h!]
        \small
        \centering
        \caption{System Functional Requirements}
        \label{tab:FR}
        \begin{tabular}{@{}lp{3cm}p{5cm}p{4cm}@{}}
        \toprule
        \textbf{ID} &
          \textbf{Requirement} &
          \textbf{Description} &
          \textbf{User Requirement} \\ \midrule
        FR-01 &
          Image Quality Enhancement &
          The model shall enhance SNR and PSNR to improve overall image clarity. &
          UR-01 \\
        FR-02 &
          Anatomical Detail Preservation &
          The model shall optimise \gls{SSIM} to ensure critical anatomical structures are preserved. &
          UR-02 \\
        FR-03 &
          System Compatibility &
          The model shall integrate with \gls{DICOM}-compliant medical imaging software, such as DVS\textsuperscript{\textregistered} by LODOX\textsuperscript{\textregistered}. &
          UR-03 \\
        FR-04 &
          Efficient Processing &
          The model shall denoise a standard LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} image within 5 seconds. &
          UR-04 \\
        FR-05 & User Interface Design  & The model shall provide a \gls{GUI} for uploading, processing, and downloading images with minimal steps.          & UR-05 \\
        FR-06 & Scalability Features   & The model shall support various input image resolutions and dimensions, adjusting algorithms accordingly.    & UR-06 \\
        FR-07 & Noise Level Adaptation & The model shall adaptively filter images with different noise levels to maintain high-quality output.        & UR-07 \\
        FR-08 & Artifact Minimisation  & The model shall include techniques to detect and minimise artifacts introduced during the denoising process. & UR-08 \\ \bottomrule
        \end{tabular}
        \end{table}

\section{Research Design}
\subsection{Familiarisation With Medical Imaging Denoising Techniques}
Relevant literature detailing the various algorithms used in X-ray image denoising was explored, with a particular focus on methods that are effective in denoising photon-limited images with Poisson noise as the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} images meet that criterion. Deep learning methods were given precedence as they are adaptable across various noise types and outperform most classical filters when trained sufficiently. In exploring the deep learning denoising models focus was on unsupervised models i.e that do not require clean images for denoising. This was covered in detail in Chapter \ref{ch:LitReview}. 

\subsection{Preliminary Data Collection}
Preliminary data was collected to familiarise ourselves with the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} system, given it was our first time working with the scanner. Additionally, this data helped in selecting appropriate phantoms to be used to train the ML models. This was through evaluating the performance of classical filters namely \gls{NLM},\gls{BM3D} and Wavelet at default settings on the data using \gls{SNR} and \gls{PSNR}.  Phantom images that most of the classical filters struggled to denoise were then selected to be used for training. This ensured that only noise-rich phantom images could be used to train the Deep-learning models.

\subsection{Model Selection and Preliminary Testing}
\label{sec:modsec}
Two critical factors guided the selection of the models to be used, i.e adaptability to learn to denoise across various noise types and not requiring clean images to learn denoising. Additionally, previous tests or applications of the models on medical image denoising or photon-limited image settings such as microscopy images was considered. The two models that met that criteria were \gls{N2N} and \gls{N2V}. They both use noisy image inputs to learn the denoising as was detailed in \ref{sec:deeplearning}. Moreover, \gls{N2N} had previously been tested on denoising \gls{MRI}. Although \gls{MRI} is predominantly affected by Rician noise, \gls{N2N} is highly adaptable and can learn to denoise photon-limited images with sufficient training. In contrast, \gls{N2V} was predominantly tested on microscopy images that are degraded by Poisson noise. Although, this is not the same condition as in LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}
 given the high levels of Poisson noise, \gls{N2V} is highly adaptable and has the potential to effectively denoise LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}
images. 

These two models were then assessed using the preliminary data collected to understand how they perform and how various parameters can be adjusted further to enhance their performance on the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}
images. This involved training them for 10 epochs with 10 iterations per epoch using only a single image for \gls{N2V} and two images for \gls{N2N}. 

\subsection{Data Collection} \label{sec:datacolelction}
\textbf{Phantom Types} \label{sec:phantom} \\
Primarily, two classes of phantoms were used in this study, as discussed below:
\begin{enumerate}
    \item \textbf{Medical Calibration Phantoms: } Used for \gls{CT} scans and bone density measurements. These phantoms are the clsest to human anatomy and are used to calibrate the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} system.
    \item \textbf{Everyday Objects:}  Include keyboards, calculators, metallic rulers, glass funnels, etc., used to model various noise scenarios and provide noise-rich images for model training.
\end{enumerate}

\textbf{Phantom Selection Criteria} \\
The following criteria were used in selecting phantoms for scans:
\begin{enumerate}
    \item \textbf{Affordability and availability:} only phantoms that were freely available to my supervisor and those provided by LODOX\textsuperscript{\textregistered} were used. This meant that anatomical phantoms were not available.
    \item \textbf{Artifacts Generation: }phantoms that were highly known to introduce artfiacts were used to deliberately introduce artifacts in the images so that the model could learn to remove such from the images.
    \item \textbf{Presence of edges:} since most denoising models struggle with preserving edges, phantoms with nice and straight edges were also incorporated to aid in model training and test the trained model's ability to preserve edges, as this is a critical part of denoising.
\end{enumerate}

\textbf{Data Collection Procedure} \\
Informed from findings from \ref{sec:phantom}, new phantom scans of the noise-rich objects were scanned. This included the following phantoms scanned from various angles:

\begin{enumerate}
    \item A Variety of tools available at the lab
    \item Level
    \item Keyboard
    \item Computer Mouse
    \item Calculator
    \item Smartphone
\end{enumerate}

The following scanning conditions were employed:

\begin{enumerate}
    \item The scanning was done strictly during the day
    \item The following parameters were set on the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} scanner for the scanning as detailed in Table \ref{tab:scansettings} below:
\end{enumerate}

\begin{table}[h!]
    \centering
    \caption{LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} scanner configuration}
    \label{tab:scansettings}
    \begin{tabular}{@{}ll@{}}
    \toprule
    \textbf{Metric/Parameter} & \textbf{Setting Used} \\ \midrule
    Procedure                 & Chest(lung) AP        \\
    Scan Time                 & 0.00 ms               \\
    Exposure Time             & 66.00 ms              \\
    X-ray tube current        & 50.00 mA              \\
    X-ray Tube Voltage        & 120 kV(KVP)           \\
    Detector Binning          & 2                     \\
    Scan Velocity             & 140 mm/s              \\
    Spatial Resolution        & 5.00 lp/mm            \\ \bottomrule
    \end{tabular}
\end{table}


After the scans, all the processing done by the DVS\textsuperscript{\textregistered} software by LODOX\textsuperscript{\textregistered} was removed by unchecking the Lucid(R) and Denoise filters applied in the processing history menu, as shown in Figure \ref{fig:menu} below. This ensured raw, unprocessed images were exported for model training. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.64\linewidth]{3_Chapters//3_Chapter_Methodology//Figures/Filteruncheck.png}
    \caption{Illustration of unchecking the custom filters in the DVS\textsuperscript{\textregistered} processing history menu}
    \label{fig:menu}
\end{figure}

A total of 10 images were collected for training as shown in Figure \ref{fig:trainingset} below:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{3_Chapters//3_Chapter_Methodology//Figures/Training Images grid.png}
    \caption{Set of training images collected}
    \label{fig:trainingset}
\end{figure}


\section{Research Instruments}
\subsection{Software}
The software was crucial for two parts of the process. First, software was needed to interface with the LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} scanner to collect data. The software used to interface with the scanner was the DVS\textsuperscript{\textregistered} by LODOX\textsuperscript{\textregistered} Systems Pty Limited. It was also used to get the raw image and export it into \gls{DICOM} file format for denoising in other software platforms. 

Secondly, software was needed to develop the denoising model.The choice of programming language to develop the denoising model is critical to ensure the model is effectively implemented, scalable, and maintainable while providing access to necessary libraries and tools for medical image processing. Per the user requirements outlined in section \ref{sec:reqs} above, the following programming languages were considered: Python and MATLAB\textsuperscript{\textregistered}. The criteria for choosing a programming language for implementation were : 

\begin{itemize}
	\item Ease of development
	\item Community support
	\item Integration with deep learning frameworks
	\item \gls{DICOM} Support
	\item Integration and Interoperability
\end{itemize}

A detailed comparison is shown in Table \ref{tab:progcomp} below:

\begin{table}[h!]
	\centering
	\caption{Comparison of Python and MATLAB\textsuperscript{\textregistered}}
	\label{tab:progcomp}
	\begin{tabular}{@{}p{2.9cm}p{5cm}p{5cm}@{}}
		\toprule
		\textbf{Criteria} &
		\textbf{Python} &
		\textbf{MATLAB\textsuperscript{\textregistered}} \\ \midrule
		Deep Learning Frameworks &
		Extensive library support for deep learning (TensorFlow, PyTorch, Keras) &
		Built-in tools for image processing and analysis (Deep Learning Toolbox, Image Processing Toolbox). \\
		\gls{DICOM} Support &
		Strong support for medical imaging (pydicom, SimpleITK) &
		Robust support for \gls{DICOM} and medical image processing. \\
		Community support &
		A large, active community with vast resources makes finding help and solutions easier. &
		Strong academic support but a smaller community compared to Python for deep learning. \\
		Ease of development &
		Fast prototyping with a vast array of pre-built models and functions available. &
		Very efficient for initial development and testing with built-in functions for common tasks. \\
		Integration and Interoperability &
		Integrates well with other software and tools, making it highly flexible. &
		Excellent integration with hardware and other MathWorks products.
	\end{tabular}
\end{table}

Python is chosen as the primary programming language for implementing the two denoising model due to its extensive library support, ease of use, and strong community resources as depicted in Table \ref{tab:progcomp} above. 


\subsection{Hardware}
The LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} scanner model was used to do the scans. It is housed at the Medical Imaging Lab, UCT HUB, Division of Biomedical Engineering. The other hardware used were the phantoms that were being scanned to provide images for model training, validation and testing as was detailed in section \ref{sec:datacolelction}. 



\section{Performance metrics}
The following groups of performance metrics were used to measure the performance metrics: Proxy quantitative, qualitative, and no-reference quality and are discussed in detail below:


\subsection{Proxy Quantitative Analysis}
Traditional metrics like \gls{SNR}, \gls{PSNR} and \gls{SSIM} cannot be used directly as there is no access to clean images; thus, proxy versions of these metrics were used by treating the denoised image as a proxy for the clean image and comparing it with the original noisy input. This will give relative noise performance. These proxy measurements are discussed in Table \ref{tab:metrics} below:

\begin{table}[h!]
	\centering
	\caption{Proxy Quantitative Analysis}
	\label{tab:metrics}
	\begin{tabular}{@{}lp{10cm}@{}}
		\toprule
		\textbf{Metric}         & \textbf{Importance}                                                                                                              \\ \midrule
		pseudo-\gls{SNR}              & Quantifies the noise level relative to image signal with higher \gls{SNR} values indicating beer noise suppression                     \\
		pseudo-\gls{PSNR}             & Measures the peak error between the original and denoised images, with higher values indicating better performance.              \\
		pseudo-\gls{SSIM} &
		Evaluates the similarity between the original and denoised images based on luminance, contrast, and structure. It is vital to ensure that the structural integrity of  LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered} images is maintained. \\
		Noise Residual Analysis & Analyses the difference between the noisy input and denoised output to assess the efficacy of the model in denoising the image. \\ \bottomrule
	\end{tabular}
\end{table}

\subsection{Qualitative Metrics}
Drawing from \cite{juneja_denoising_2024} quality analysis, the denoised images were visually compared against the processed output from the DVS\textsuperscript{\textregistered} software using an eye test to assess image quality. Additionally, the denoised images were compared to their noisy counterparts to evaluate differences in clarity, contrast, and overall usability. This visual assessment helps to identify improvements in image quality and ensures that the denoising process enhances the diagnostic value of the images without introducing artifacts or losing critical information.


\subsection{No-Reference Quality Metrics}
The following no-reference metrics were used to evaluate the image quality since they were inherently

designed to assess image quality without needing a clean reference image. The following no-reference metrics were used:

\begin{itemize}
	\item \gls{NIQE}: This model evaluates image quality by measuring deviations from statistical regularities found in natural images.
	\item \gls{BRISQUE}: calculates quality using features that capture natural scene statistics.
	\item \gls{PIQE}: computes image quality based on perceptual information in localised spatial regions of the image.
\end{itemize}

\section{Limitations}
\begin{enumerate}
	\item \textbf{Use of Non-Anatomical Phantoms}: The study utilizes non-clinical phantoms that do not accurately model human tissue. This limitation may hinder the direct application of findings to clinical settings, as the phantoms lack the anatomical complexities present in actual human subjects, potentially impacting the generalisability of the results.
	\item \textbf{Time Constraints on Model Training}: The machine learning models were trained within the confines of a 12-week project duration. Extended training time could have improved the efficacy and performance of these models. The limited training period may not have allowed the models to be fully optimised, potentially affecting the robustness of the denoising outcomes.
	\item \textbf{Limited Scope of Unsupervised Models}: The methodology considers only two unsupervised denoising models. While there is potential for better performance with additional models, the study is constrained by the availability and maturity of unsupervised techniques. As discussed in Chapter 2, most advanced denoising models currently rely on supervised learning, which limits the exploration of a broader range of unsupervised approaches.
\end{enumerate}
These limitations highlight areas where the methodology could be refined in future studies, such as the inclusion of more realistic phantoms, extended training periods, and the exploration of a wider variety of unsupervised denoising models.


\section{Ethics}
Ethics was complied with as set out and specified by the UCT Ethics guideline and guidelines stipulated by the EBE faculty. No human subjects are used in the research, and the use of Artificial Intelligence in this research qualifies as minimal risk. A detailed Ethics approval and application form is detailed in Appendix B. 



\section{Conclusion}
In conclusion, this chapter provided a comprehensive overview of the research methodology employed in developing the denoising model for LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}images. The research design was carefully structured, beginning with exploring relevant literature and progressing through preliminary data collection, model selection, and the rigorous testing and validation of the chosen models. The methodology emphasised the importance of selecting appropriate phantoms and utilising both classical and advanced deep learning techniques to achieve the desired denoising outcomes. The software and hardware tools were carefully chosen to ensure compatibility and efficiency, with Python emerging as the preferred language due to its extensive library support and scalability in machine learning tasks.

The analysis phase utilised a combination of quantitative metrics, such as \gls{SNR}, \gls{PSNR}, and \gls{SSIM}, alongside qualitative assessments to validate the effectiveness of the denoising models. Despite the limitations of using non-anatomical phantoms and the constrained training period, the study successfully demonstrated the potential of unsupervised deep learning models, particularly \gls{N2N} and \gls{N2V}, in enhancing the quality of LODOX\textsuperscript{\textregistered} Statscan\textsuperscript{\textregistered}images. 

