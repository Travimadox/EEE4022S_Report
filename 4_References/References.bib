% MASc & PhD Thesis Template

% Department of Electrical & Computer Engineering (ECE)
% Queen's University 

% Requisitioned By: Professor Michael Greenspan
% Prepared By: Ian Maquignaz M.Eng, MASc

% Template Version 1.0 - September 2017

% >>> OPEN THIS FILE IN TeXstudio TO ADD REFERENCES TO YOUR THESIS! <<< %

% -> FILE DESCRIPTION :: References.bib
%	This file contains all of your references in bibtex format. 
% 	There are a couple methods of entering references:
%		1. Copy and past the BibTex reference directly from Google Schoolar!
%		2. Use a plugin to generate the citation for you! (See Chrome Extension 'BibTex Entry From Url')
%		3. Manual Entry

% Everything outside a bibtex "block" is a comment so none of this text affects the file. Use this to your advantage write your notes about papers in here right after the bibtex block!
	

%-------------------------------------------------------------------------------------------------------------
% LODOX REFERENCES
%-------------------------------------------------------------------------------------------------------------



@misc{lodoxsystems_lodox_2018,
  author       = {{Lodox Systems}},
  title        = {Lodox {Technology} {Explained}},
  year         = {2018},
  month        = {August},
  note         = {Accessed: August 8, 2024. [Online Video]. Available: \url{https://www.youtube.com/watch?v=_zVocRzl4v4}}
}

@article{beningfield_report_2003,
	title = {Report on a new type of trauma full-body digital {X}-ray machine},
	volume = {10},
	issn = {1438-1435},
	url = {https://doi.org/10.1007/s10140-003-0271-x},
	doi = {10.1007/s10140-003-0271-x},
	abstract = {The purpose of this study was to evaluate the diagnostic equivalence, radiation dose, clinical usefulness and radiographic aspects of a low-dose, full-body digital X-ray machine in a busy trauma unit. A digital trauma X-ray machine known as "LODOX" was compared with conventional radiography between June 1999 and November 2001 in the Groote Schuur Hospital Trauma Unit, Cape Town. Digital images of a variety of body regions commonly imaged in trauma were compared for diagnostic image quality in a number of categories with equivalent conventional radiographs. A seven-point equivalence scoring system ranging from much inferior (−3) through equivalent (0) to much superior (+3) was used in each category. Radiation dose was recorded and compared with that in conventional measurements. Turnaround times of patients undergoing digital and conventional X-rays were evaluated. Clinical and radiographic issues were assessed by staff feedback. The digital images when compared with conventional film had an overall mean equivalence score of −0.429, with a standard deviation (SD) of 0.77. The best digital performance was in the mediastinum (mean 0.346, SD 0.49) and the weakest was for bony detail (mean −0.654, SD 0.81). Relative digital radiation dose compared to conventional varied from 72\% (chest) to 2\% (pelvis), with a simple average of 6\%. Radiographic points included full-body imaging capability and differing positioning, penetration, workflow and practicality considerations. The digital images required overall patient times of 5–6 min, compared with 8–48 min for conventional X-rays. New installations are under way, and computed tomography and angiography applications are being explored. FDA approval is awaited. Projected cost is similar to that of flat-panel digital units. This digital unit was felt to be diagnostically substantially equivalent to conventional radiographs, with low-dose full-body imaging, improved workflow, digital technology and long-term cost benefits as potentially favourable contributions to trauma imaging.},
	language = {en},
	number = {1},
	urldate = {2024-07-31},
	journal = {Emergency Radiology},
	author = {Beningfield, S. and Potgieter, H. and Nicol, A. and van As, S. and Bowie, G. and Hering, E. and Lätti, E.},
	month = apr,
	year = {2003},
	keywords = {Digital radiography, Multiple trauma, Radiographic image enhancement, Trauma centres, X-ray computed tomography scanners},
	pages = {23--29},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\VRJL7U4V\\Beningfield et al. - 2003 - Report on a new type of trauma full-body digital X.pdf:application/pdf},
}

@article{exadaktylos_total-body_2008,
	title = {Total-body digital {X}-ray in trauma: {An} experience report on the first operational full body scanner in {Europe} and its possible role in {ATLS}},
	volume = {39},
	issn = {0020-1383},
	shorttitle = {Total-body digital {X}-ray in trauma},
	url = {https://www.sciencedirect.com/science/article/pii/S0020138307004275},
	doi = {10.1016/j.injury.2007.10.019},
	abstract = {When patients enter our emergency room with suspected multiple injuries, Statscan provides a full body anterior and lateral image for initial diagnosis, and then zooms in on specific smaller areas for a more detailed evaluation. In order to examine the possible role of Statscan in the management of multiply injured patients we implemented a modified ATLS® algorithm, where X-ray of C-spine, chest and pelvis have been replaced by single-total a.p./lat. body radiograph. Between 15 October 2006 and 1 February 2007 143 trauma patients (mean ISS 15+/−14 (3–75)) were included. We compared the time in resuscitation room to 650 patients (mean ISS 14+/−14 (3–75)) which were treated between 1 January 2002 and 1 January 2004 according to conventional ATLS protocol. The total-body scanning time was 3.5min (3–6min) compared to 25.7 (8–48min) for conventional X-rays, The total ER time was unchanged 28.7min (13–58min) compared to 29.1min (15–65min) using conventional plain radiography. In 116/143 patients additional CT scans were necessary. In 98/116 full body trauma CT scans were performed. In 18/116 patients selective CT scans were ordered based on Statscan findings. In 43/143 additional conventional X-rays had to be performed, mainly due to inadequate a.p. views of fractured bones. All radiographs were transmitted over the hospital network (Picture Archiving and Communication System, PACS) for immediate simultaneous viewing at different places. The rapid availability of images for interpretation because of their digital nature and the reduced need for repeat exposures because of faulty radiography are also felt to be strengths.},
	number = {5},
	urldate = {2024-07-31},
	journal = {Injury},
	author = {Exadaktylos, A. K. and Benneker, L. M. and Jeger, V. and Martinolli, L. and Bonel, H. M. and Eggli, S. and Potgieter, H. and Zimmermann, H.},
	month = may,
	year = {2008},
	keywords = {ATLS, Full body X-ray, Lodox, Trauma},
	pages = {525--529},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\AHVTXLUY\\S0020138307004275.html:text/html},
}

@article{fu_lodoxstatscan_2011,
	title = {Lodox/{Statscan} provides benefits in evaluation of gunshot injuries},
	volume = {29},
	issn = {0735-6757},
	url = {https://www.sciencedirect.com/science/article/pii/S0735675711001331},
	doi = {10.1016/j.ajem.2011.04.001},
	number = {7},
	urldate = {2024-07-31},
	journal = {The American Journal of Emergency Medicine},
	author = {Fu, Chih-Yuan and Wang, Yu-Chun and Hsieh, Chi-Hsun and Chen, Ray-Jade},
	month = sep,
	year = {2011},
	pages = {823--827},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\KS4439A5\\S0735675711001331.html:text/html},
}

@article{fu_lodoxstatscan_2008,
	title = {Lodox/{Statscan} provides rapid identification of bullets in multiple gunshot wounds},
	volume = {26},
	issn = {0735-6757},
	url = {https://www.sciencedirect.com/science/article/pii/S0735675708001265},
	doi = {10.1016/j.ajem.2008.01.047},
	abstract = {Lodox/Statscan is a new digital radiograph machine with low-dose radiation exposure. It provides rapid and whole-body scans. In multiple gunshot trauma, the conventional radiograph or computed tomography scan might be time consuming, and the resuscitation might be paused temporarily. We applied the Lodox/Statscan to search bullets within a short period, and the resuscitation was nonstop during imaging study. As a result, it serves as a valuable adjunct in providing rapid identification of metal projectiles in multiple gunshot wounds.},
	number = {8},
	urldate = {2024-07-31},
	journal = {The American Journal of Emergency Medicine},
	author = {Fu, Chih-Yuan and Wu, Shih-Chi and Chen, Ray-Jade},
	month = oct,
	year = {2008},
	pages = {965.e5--965.e7},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\ESJD8VZW\\S0735675708001265.html:text/html},
}

@article{boffard_use_2006,
	title = {The {Use} of {Low} {Dosage} {X}-{Ray} ({Lodox}/{Statscan}) in {Major} {Trauma}: {Comparison} {Between} {Low} {Dose} {X}-{Ray} and {Conventional} {X}-{Ray} {Techniques}},
	volume = {60},
	issn = {2163-0755},
	shorttitle = {The {Use} of {Low} {Dosage} {X}-{Ray} ({Lodox}/{Statscan}) in {Major} {Trauma}},
	url = {https://journals.lww.com/jtrauma/fulltext/2006/06000/the_use_of_low_dosage_x_ray__lodox_statscan__in.4.aspx},
	doi = {10.1097/01.ta.0000220393.26629.6c},
	abstract = {Background: 
          Patients presenting with major trauma normally require resuscitation, usually carried out according to techniques laid down by the Advanced Trauma Life Support (ATLS) Program of the American College of Surgeons. Techniques normally suggested include the routine radiology of the cervical spine, chest and pelvis. This can sometimes be time consuming and may not return helpful information in all cases.
          Methods: 
          This paper describes the use of a new low dose X-ray technique (Lodox/Statscan to perform these X-rays, and compares their interpretation by both radiologists and trauma surgeons with images obtained from conventional X-rays. The time taken for the respective images to be obtained was measured.
          Results: 
          There was no difference in the amount of information obtained.
          Conclusion: 
          The use of the Lodox allowed a substantial reduction in the time taken for resuscitation, without prejudice to diagnostic radiology.},
	language = {en-US},
	number = {6},
	urldate = {2024-07-31},
	journal = {Journal of Trauma and Acute Care Surgery},
	author = {Boffard, Kenneth D. and Goosen, Jacques and Plani, Frank and Degiannis, Elias and Potgieter, Herman},
	month = jun,
	year = {2006},
	pages = {1175},
	file = {Snapshot:C\:\\Users\\User\\Zotero\\storage\\KX9RZNL5\\the_use_of_low_dosage_x_ray__lodox_statscan__in.4.html:text/html},
}

@article{mulligan_initial_2006,
	title = {Initial experience with {Lodox} {Statscan} imaging system for detecting injuries of the pelvis and appendicular skeleton},
	volume = {13},
	issn = {1438-1435},
	url = {https://doi.org/10.1007/s10140-006-0525-5},
	doi = {10.1007/s10140-006-0525-5},
	abstract = {The purpose of this study is to assess detection of pelvic and extremity injuries employing a rapid whole-body imaging system (Lodox Statscan). We retrospectively reviewed 37 consecutive cases. The study was approved by our hospital review board and carried out with Health Insurance Portability and Accountability Act (HIPAA) compliance. Anterior–posterior (AP) whole-body Lodox Statscan (LS) exams, obtained in 10–13 s, were reviewed by two musculoskeletal radiologists. Each patient’s concurrent computed radiographs (CR) and computed tomographic (CT) studies were reviewed at a later date. There were 23 males and 14 females; ages ranged from 14 to 103 years (average 40 years). Sixteen patients had a total of 73 abnormalities seen on LS, CR or CT. The LS examination identified 47 abnormalities (64.4\%) in 15 patients. Twenty-six additional abnormalities were detected with the other modalities (CR, CT) in 11 patients; 8 were evident on LS images in retrospect, with the remaining 18 not identified even retrospectively. Of these retrospectively occult injuries, only one was deemed significant to the acute management of the patient. The LS imaging system seems to be a useful tool for rapid screening of multitrauma patients.},
	language = {en},
	number = {3},
	urldate = {2024-07-31},
	journal = {Emergency Radiology},
	author = {Mulligan, Michael E. and Flye, Christopher W.},
	month = dec,
	year = {2006},
	keywords = {Emergency radiology, Fractures, Skeleton},
	pages = {129--133},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\HPX283VN\\Mulligan and Flye - 2006 - Initial experience with Lodox Statscan imaging sys.pdf:application/pdf},
}

@article{amirlak_novel_2009,
	title = {Novel use of {Lodox}® {Statscan}® in a level one trauma center},
	volume = {15},
	language = {en},
	number = {6},
	journal = {Ulus Travma Acil Cerrahi Derg},
	author = {Amirlak, Bardia and Zakhary, Bishoy and Weichman, Katie and Ahluwalia, Hardeep and Forse, Armour R and Gaines, Ray D},
	year = {2009},
	file = {Amirlak et al. - 2009 - Novel use of Lodox® Statscan® in a level one traum.pdf:C\:\\Users\\User\\Zotero\\storage\\PFG7GLU3\\Amirlak et al. - 2009 - Novel use of Lodox® Statscan® in a level one traum.pdf:application/pdf},
}

@misc{noauthor_full-body_nodate,
	title = {Full-body radiography ({LODOX} {Statscan}) in trauma and emergency medicine: a report from the first {European} installation site},
	shorttitle = {Full-body radiography ({LODOX} {Statscan}) in trauma and emergency medicine},
	url = {https://journals.sagepub.com/doi/epdf/10.1177/1460408610382493},
	language = {en},
	urldate = {2024-07-31},
	doi = {10.1177/1460408610382493},
	file = {Snapshot:C\:\\Users\\User\\Zotero\\storage\\XALRGRML\\1460408610382493.html:text/html},
}

@article{whiley_full-body_2013,
	title = {Full-{Body} {X}-{Ray} {Imaging} to {Facilitate} {Triage}: {A} {Potential} {Aid} in {High}-{Volume} {Emergency} {Departments}},
	volume = {2013},
	copyright = {Copyright © 2013 S. P. Whiley et al.},
	issn = {2090-2859},
	shorttitle = {Full-{Body} {X}-{Ray} {Imaging} to {Facilitate} {Triage}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2013/437078},
	doi = {10.1155/2013/437078},
	abstract = {The levels of traumatic injury seen in South African emergency departments (EDs) are epidemic. This is coupled with a severe lack of resources and adequately trained emergency staff. The Lodox Statscan (LS) is an X-ray scanner capable of producing rapid, low-dose, and full-body X-ray images. In this paper, a new trauma protocol—the Johannesburg trauma protocol—that implements LS scanning on entry to the ED as a triage tool is reported. A case study illustrating the use of LS to triage 63 patients in a single Saturday shift at a level 1 Trauma Centre is also presented. Because of the ability to rapidly and safely provide X-ray imaging information to support clinical decision making, the LS could be a useful tool to aid in resource allocation to improve treatment of the high levels of trauma patients that present to South African EDs daily.},
	language = {en},
	number = {1},
	urldate = {2024-07-31},
	journal = {Emergency Medicine International},
	author = {Whiley, S. P. and Alves, H. and Grace, S.},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2013/437078},
	pages = {437078},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\NHB6Z6IA\\Whiley et al. - 2013 - Full-Body X-Ray Imaging to Facilitate Triage A Po.pdf:application/pdf;Snapshot:C\:\\Users\\User\\Zotero\\storage\\PADVNKBR\\437078.html:text/html},
}

@article{evangelopoulos_personal_2009,
	title = {Personal experience with whole-body, low-dosage, digital {X}-ray scanning ({LODOX}-{Statscan}) in trauma},
	volume = {17},
	issn = {1757-7241},
	url = {https://doi.org/10.1186/1757-7241-17-41},
	doi = {10.1186/1757-7241-17-41},
	abstract = {Lodox-Statscan is a whole-body, skeletal and soft-tissue, low-dose X-ray scanner Anterior-posterior and lateral thoraco-abdominal studies are obtained in 3-5 minutes with only about one-third of the radiation required for conventional radiography. Since its approval by the Food and Drug Administration (FDA) in the USA, several trauma centers have incorporated this technology into their Advanced Trauma Life Support protocols. This review provides a brief overview of the system, and describes the authors' own experience with the system.},
	number = {1},
	urldate = {2024-07-31},
	journal = {Scandinavian Journal of Trauma, Resuscitation and Emergency Medicine},
	author = {Evangelopoulos, Dimitrios S. and Deyle, Simone and Zimmermann, Heinz and Exadaktylos, Aristomenis K.},
	month = sep,
	year = {2009},
	keywords = {Conventional Radiograph, Conventional Radiography, Entry Dose, Skin Entry Dose, United Nations Scientific Committee},
	pages = {41},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\LB7AV6J3\\Evangelopoulos et al. - 2009 - Personal experience with whole-body, low-dosage, d.pdf:application/pdf;Snapshot:C\:\\Users\\User\\Zotero\\storage\\2SSH9US3\\1757-7241-17-41.html:text/html},
}

@misc{marais_lodox_nodate,
	title = {Lodox {Critical} {Imaging} {Technology}},
	url = {https://lodox.com/},
	abstract = {Lodox Systems is a global leader in producing full body X-ray imaging devices for medical use in trauma and forensic pathology centres.},
	language = {en-US},
	urldate = {2024-08-01},
	journal = {Lodox Systems},
	author = {Marais, Carin},
	file = {Snapshot:C\:\\Users\\User\\Zotero\\storage\\QPH9L4NK\\lodox.com.html:text/html},
}

@misc{lodoxsystems_lodox_2018-1,
  author       = {{Lodox Systems}},
  title        = {Lodox Solution for Trauma Units},
  year         = {2018},
  month        = {August},
  note         = {Accessed: August 8, 2024. [Online Video]. Available: \url{https://www.youtube.com/watch?v=M6gmVEs5wMs}}
}


@misc{lodoxsystems_lodox_2017,
  author       = {{Lodox Systems}},
  title        = {Lodox eXero-dr Animation},
  year         = {2017},
  month        = {October},
  note         = {Accessed: August 8, 2024. [Online Video]. Available: \url{https://www.youtube.com/watch?v=uW2xlsGQYhw}}
}


@phdthesis{PhDMattieu,
  author       = {Mattieu De Villiers},
  title        = {Limited Angle Tomography},
  school       = { Dept. Elect. Eng., Univ. of Cape Town},
  year         = 2004,
  address      = {Cape Town, South Africa},
  month        = {June},
  type         = {Ph.D. dissertation},
  note         = {[Online]. Available: \url{http://www.dip.ee.uct.ac.za/publications/theses/PhDMattieu.pdf}}
}


@masterthesis{MScFrank,
  title        = {Automated 3D reconstruction of Lodox Statscan images for forensic application},
  author       = {Frank Bolton},
  year         = 2011,
  month        = {July},
  address      = {Cape Town, South Africa},
  note         = {[Online]. Available: \url{http://www.dip.ee.uct.ac.za/publications/theses/MScFrank.pdf}},
  school       = {Dept. Elect. Eng., Univ. of Cape Town},
  type         = {Master's thesis}
}


@misc{ProjectBrief,
author = {Dr. Yaaseen Martin},
title = {Project Brief},
howpublished = {\url{https://amathuba.uct.ac.za}},
month = {},
year = {},
note = {(Accessed on 10/20/2024)}
}


%-------------------------------------------------------------------------------------------------------------
% X-RAY THEORY REFERENCES
%-------------------------------------------------------------------------------------------------------------


@incollection{buzug_milestones_2008,
	address = {Berlin, Heidelberg},
	title = {Milestones of {Computed} {Tomography}},
	isbn = {978-3-540-39408-2},
	url = {https://doi.org/10.1007/978-3-540-39408-2_3},
	abstract = {Conventional X-ray imaging suffers fromthe severe drawback that it only produces two-dimensional projections of a three-dimensional object. This results in a reduction in spatial information (although an experienced radiologist might be able to compensate for this). In any case, a projection represents an averaging. The result of the averaging can be imagined if one were to overlay several radiographic sections at the light box for diagnosis. It would be difficult for even an expert to interpret the results, as averaging comes along with a considerable reduction in contrast, compared with the contrast present in one slice.},
	language = {en},
	urldate = {2024-07-27},
	booktitle = {Computed {Tomography}: {From} {Photon} {Statistics} to {Modern} {Cone}-{Beam} {CT}},
	publisher = {Springer},
	editor = {Buzug, Thorsten},
	year = {2008},
	doi = {10.1007/978-3-540-39408-2_3},
	pages = {75--99},
}

@incollection{haidekker_trends_2013,
	address = {New York, NY},
	title = {Trends in {Medical} {Imaging} {Technology}},
	isbn = {978-1-4614-7073-1},
	url = {https://doi.org/10.1007/978-1-4614-7073-1_7},
	abstract = {Medical imaging technologies have, to a varying extent, experienced significant recent progress. From the introduction of a new imaging modality to its adoption to routine clinical practice, many years of development and testing are needed. Recently, the focus of medical imaging research shifted toward detail optimization and the development of new disease-specific protocols, although several striking new developments need to be highlighted. For example, the adaptation of phase and darkfield contrast, well-known from microscope imaging, to X-ray imaging, provides a new and astonishing level of contrast in X-ray imaging and CT. Another example is the development of new, ultra-portable ultrasound devices, which make ultrasound even more attractive as a fast and low-cost imaging modality. Laser-based optical imaging that uses visible or near-infrared light deserves special attention as some methods have been adopted in clinical practice. Last but not least, improved image processing—both in terms of new algorithms and of improved computing power—have continually improved image quality and opened new avenues of image processing, with many new functions available to aid the radiologist in providing a diagnosis.},
	language = {en},
	urldate = {2024-08-12},
	booktitle = {Medical {Imaging} {Technology}},
	publisher = {Springer},
	author = {Haidekker, Mark A.},
	editor = {Haidekker, Mark A},
	year = {2013},
	doi = {10.1007/978-1-4614-7073-1_7},
	pages = {111--119},
}

@incollection{haidekker_x-ray_2013,
	address = {New York, NY},
	title = {X-{Ray} {Projection} {Imaging}},
	isbn = {978-1-4614-7073-1},
	url = {https://doi.org/10.1007/978-1-4614-7073-1_2},
	abstract = {X-ray imaging is the oldest medical imaging modality, which found its way into medical practice shortly after the discovery of the X-rays in 1895. X-ray imaging is a projection technique, and image formation takes place traditionally on photosensitive film, although direct digital X-ray imaging is becoming more and more common. In its most common form, X-ray imaging is a qualitative modality. X-rays are high-energy photons, and atomic interaction with inner shell electrons is fundamental to both X-ray production and generation of X-ray contrast. Soft-tissue contrast is comparatively low, but bone and air provide excellent contrast. In some cases, contrast can be enhanced with contrast agents.  An undesirable (but unavoidable) side-effect of the photon-atom interaction is the ionization of tissue along the beam path, which can lead to radiation damage. X-ray images can reveal very subtle features, and its popularity is further enhanced by the relatively inexpensive equipment and the straightforward imaging procedure.},
	language = {en},
	urldate = {2024-08-12},
	booktitle = {Medical {Imaging} {Technology}},
	publisher = {Springer},
	author = {Haidekker, Mark A.},
	editor = {Haidekker, Mark A},
	year = {2013},
	doi = {10.1007/978-1-4614-7073-1_2},
	pages = {13--35},
}

@incollection{haidekker_introduction_2013,
	address = {New York, NY},
	title = {Introduction},
	isbn = {978-1-4614-7073-1},
	url = {https://doi.org/10.1007/978-1-4614-7073-1_1},
	abstract = {“Medical imaging refers to several different technologies that are used to view the human body in order to diagnose, monitor, or treat medical conditions”. All imaging modalities have in common that the medical condition becomes visible by some form of contrast, meaning that the feature of interest (such as a tumor) can be recognized in the image and examined by a trained radiologist. The image can be seen as a model of the imaged tissue. Images in the context of this book are digital. This implies a finite resolution with the pixel as the smallest element. Furthermore, all imaging modalities lead to some degradation of the image when compared to the original object. Primarily, the degradation consists of blur (loss of detail) and noise (unwanted contrast). Some underlying principles are common to all imaging modalities, such as the interpretation as a system and its mathematical treatment. The image itself can be seen as a multidimensional signal. In many cases, the steps in image formation can be seen as linear systems, which allow simplified mathematical treatment.},
	language = {en},
	urldate = {2024-08-12},
	booktitle = {Medical {Imaging} {Technology}},
	publisher = {Springer},
	author = {Haidekker, Mark A.},
	editor = {Haidekker, Mark A},
	year = {2013},
	doi = {10.1007/978-1-4614-7073-1_1},
	pages = {1--12},
}


%-------------------------------------------------------------------------------------------------------------
% NOISE GENERAL REFERENCES
%-------------------------------------------------------------------------------------------------------------

@article{gravel_method_2004,
	title = {A method for modeling noise in medical images},
	volume = {23},
	issn = {1558-254X},
	url = {https://ieeexplore.ieee.org/abstract/document/1339429},
	doi = {10.1109/TMI.2004.832656},
	abstract = {We have developed a method to study the statistical properties of the noise found in various medical images. The method is specifically designed for types of noise with uncorrelated fluctuations. Such signal fluctuations generally originate in the physical processes of imaging rather than in the tissue textures. Various types of noise (e.g., photon, electronics, and quantization) often contribute to degrade medical images; the overall noise is generally assumed to be additive with a zero-mean, constant-variance Gaussian distribution. However, statistical analysis suggests that the noise variance could be better modeled by a nonlinear function of the image intensity depending on external parameters related to the image acquisition protocol. We present a method to extract the relationship between an image intensity and the noise variance and to evaluate the corresponding parameters. The method was applied successfully to magnetic resonance images with different acquisition sequences and to several types of X-ray images.},
	number = {10},
	urldate = {2024-08-04},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Gravel, P. and Beaudoin, G. and De Guise, J.A.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Signal processing, Additive noise, Biomedical imaging, Degradation, Design methodology, Fluctuations, Gaussian distribution, Gaussian noise, Quantization, Statistical analysis},
	pages = {1221--1232},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\User\\Zotero\\storage\\P3FX2TXS\\1339429.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\RHCYPMA8\\Gravel et al. - 2004 - A method for modeling noise in medical images.pdf:application/pdf},
}

@article{huda_radiographic_2015,
	title = {Radiographic {Techniques}, {Contrast}, and {Noise} in {X}-{Ray} {Imaging}},
	volume = {204},
	issn = {0361-803X},
	url = {https://ajronline.org/doi/full/10.2214/AJR.14.13116},
	doi = {10.2214/AJR.14.13116},
	number = {2},
	urldate = {2024-08-04},
	journal = {American Journal of Roentgenology},
	author = {Huda, Walter and Abrahams, R. Brad},
	month = feb,
	year = {2015},
	note = {Publisher: American Roentgen Ray Society},
	pages = {W126--W131},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\BM3JHM6F\\Huda and Abrahams - 2015 - Radiographic Techniques, Contrast, and Noise in X-.pdf:application/pdf},
}

@article{kim_measurements_2024,
	title = {Measurements of the noise power spectrum for digital x-ray imaging devices},
	volume = {69},
	issn = {0031-9155},
	url = {https://dx.doi.org/10.1088/1361-6560/ad1999},
	doi = {10.1088/1361-6560/ad1999},
	abstract = {Objective. The noise characteristics of digital x-ray imaging devices are determined by contributions such as photon noise, electronic noise, and fixed pattern noise, and can be evaluated from measuring the noise power spectrum (NPS), which is the power spectral density of the noise. Hence, accurately measuring NPS is important in developing detectors for acquiring low-noise digital x-ray images. To make accurate measurements, it is necessary to understand NPS, identify problems that may arise, and know how to process the obtained x-ray images. Approach. The primitive concept of NPS is first introduced with a periodogram-based estimate and its bias and variance are discussed. In measuring NPS based on the IEC62220 standards, various issues, such as the fixed pattern noise, high-precision estimates, and lag corrections, are summarized with simulation examples. Main results. High-precision estimates can be provided for an appropriate number of samples extracted from x-ray images while compromising spectral resolution. Depending on medical imaging systems, by eliminating the influence of fixed pattern noise, NPS, which represents only photon and electronic noise, can be efficiently measured. For NPS measurements in dynamic detectors, an appropriate lag correction technique can be selected depending on the emitted x-rays and image acquisition process. Significance. Various issues in measuring NPS are reviewed and summarized for accurately evaluating the noise performance of digital x-ray imaging devices.},
	language = {en},
	number = {3},
	urldate = {2024-08-04},
	journal = {Physics in Medicine \& Biology},
	author = {Kim, Dong Sik},
	month = jan,
	year = {2024},
	note = {Publisher: IOP Publishing},
	pages = {03TR01},
	file = {IOP Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\ZNRBEG72\\Kim - 2024 - Measurements of the noise power spectrum for digit.pdf:application/pdf},
}

@article{seibert_tradeoffs_2004,
	title = {Tradeoffs between image quality and dose},
	volume = {34},
	issn = {1432-1998},
	url = {https://doi.org/10.1007/s00247-004-1268-7},
	doi = {10.1007/s00247-004-1268-7},
	abstract = {Image quality takes on different perspectives and meanings when associated with the concept of as low as reasonably achievable (ALARA), which is chiefly focused on radiation dose delivered as a result of a medical imaging procedure. ALARA is important because of the increased radiosensitivity of children to ionizing radiation and the desire to keep the radiation dose low. By the same token, however, image quality is also important because of the need to provide the necessary information in a radiograph in order to make an accurate diagnosis. Thus, there are tradeoffs to be considered between image quality and radiation dose, which is the main topic of this article. ALARA does not necessarily mean the lowest radiation dose, nor, when implemented, does it result in the least desirable radiographic images. With the recent widespread implementation of digital radiographic detectors and displays, a new level of flexibility and complexity confronts the technologist, physicist, and radiologist in optimizing the pediatric radiography exam. This is due to the separation of the acquisition, display, and archiving events that were previously combined by the screen-film detector, which allows for compensation for under- and overexposures, image processing, and on-line image manipulation. As explained in the article, different concepts must be introduced for a better understanding of the tradeoffs encountered when dealing with digital radiography and ALARA. In addition, there are many instances during the image acquisition/display/interpretation process in which image quality and associated dose can be compromised. This requires continuous diligence to quality control and feedback mechanisms to verify that the goals of image quality, dose and ALARA are achieved.},
	language = {en},
	number = {3},
	urldate = {2024-08-04},
	journal = {Pediatric Radiology},
	author = {Seibert, J. Anthony},
	month = oct,
	year = {2004},
	keywords = {Digital radiography, ALARA, Image evaluation, Image quality, Radiation dose, Radiographic techniques},
	pages = {S183--S195},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\PRMMDZHC\\Seibert - 2004 - Tradeoffs between image quality and dose.pdf:application/pdf},
}

@article{veldkamp_dose_2009,
	series = {Digital {Radiography}},
	title = {Dose and perceived image quality in chest radiography},
	volume = {72},
	issn = {0720-048X},
	url = {https://www.sciencedirect.com/science/article/pii/S0720048X09003349},
	doi = {10.1016/j.ejrad.2009.05.039},
	abstract = {Chest radiography is the most commonly performed diagnostic X-ray examination. The radiation dose to the patient for this examination is relatively low but because of its frequent use, the contribution to the collective dose is considerable. Consequently, optimization of dose and image quality offers a challenging area of research. In this article studies on dose reduction, different detector technologies, optimization of image acquisition and new technical developments in image acquisition and post processing will be reviewed. Studies indicate that dose reduction in PA chest images to at least 50\% of commonly applied dose levels does not affect diagnosis in the lung fields; however, dose reduction in the mediastinum, upper abdomen and retrocardiac areas appears to directly deteriorate diagnosis. In addition to patient dose, also the design of the various digital detectors seems to have an effect on image quality. With respect to image acquisition, studies showed that using a lower tube voltage improves visibility of anatomical structures and lesions in digital chest radiographs but also increases the disturbing appearance of ribs. New techniques that are currently being evaluated are dual energy, tomosynthesis, temporal subtraction and rib suppression. These technologies may improve diagnostic chest X-ray further. They may for example reduce the negative influence of over projection of ribs, referred to as anatomic noise. In chest X-ray this type of noise may be the dominating factor in the detection of nodules. In conclusion, optimization and new developments will enlarge the value of chest X-ray as a mainstay in the diagnosis of chest diseases.},
	number = {2},
	urldate = {2024-08-04},
	journal = {European Journal of Radiology},
	author = {Veldkamp, Wouter J. H. and Kroft, Lucia J. M. and Geleijns, Jacob},
	month = nov,
	year = {2009},
	keywords = {Image quality, Chest radiography, Dose},
	pages = {209--217},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\5YN45ILM\\S0720048X09003349.html:text/html},
}

@article{manson_image_2019,
	title = {Image {Noise} in {Radiography} and {Tomography}: {Causes}, {Effects} and {Reduction} {Techniques}},
	volume = {3},
	issn = {2573-2609},
	shorttitle = {Image {Noise} in {Radiography} and {Tomography}},
	url = {http://juniperpublishers.com/ctcmi/CTCMI.MS.ID.555620.php},
	doi = {10.19080/CTCMI.2019.02.555620},
	language = {English},
	number = {4},
	urldate = {2024-08-04},
	journal = {Current Trends in Clinical \& Medical Imaging},
	author = {Manson, E. N. and Ampoh, V. Atuwo and Fiagbedzi, E. and Amuasi, J. H. and Flether, J. J. and Schandorf, C.},
	month = oct,
	year = {2019},
	note = {Publisher: Juniper Publishers},
	pages = {86--91},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\XGPL4YQW\\Manson et al. - 2019 - Image Noise in Radiography and Tomography Causes,.pdf:application/pdf},
}

@misc{warner_understanding_2020,
	title = {Understanding and {Managing} {Noise} {Sources} in {X}-ray {Imaging}},
	url = {https://www.carestream.com/blog/2020/04/21/understanding-and-managing-noise-sources-in-x-ray-imaging/},
	abstract = {Radiographers need to capture the best possible imaging exam at the lowest radiation dose. Yet the X-ray process introduces unavoidable "noise" at several times during the process. Learn more about the noise sources in X-ray imaging; and the science behind how they are transformed into pristine visuals for medical diagnosis.},
	language = {en-US},
	urldate = {2024-08-04},
	journal = {Everything Rad},
	author = {Warner, Melody},
	month = apr,
	year = {2020},
	file = {Snapshot:C\:\\Users\\User\\Zotero\\storage\\BZG8WS99\\understanding-and-managing-noise-sources-in-x-ray-imaging.html:text/html},
}

@article{goyal_noise_2018,
	title = {Noise {Issues} {Prevailing} in {Various} {Types} of {Medical} {Images}},
	volume = {11},
	url = {https://biomedpharmajournal.org/vol11no3/noise-issues-prevailing-in-various-types-of-medical-images/},
	abstract = {Introduction

Image processing is a dominant tool for many areas including robotics, biometrics, security and surveillance, remote sensing and medical imaging. The holistic applicability and performance of a particular image processing task, in fact largely depends on the quality of the test image},
	language = {en-US},
	number = {3},
	urldate = {2024-08-04},
	journal = {Biomedical and Pharmacology Journal},
	author = {Goyal, Bhawna and Dogra, Ayush and Agrawal, Sunil and Sohi, B. S.},
	month = sep,
	year = {2018},
	pages = {1227--1237},
}

@misc{noauthor_noise_nodate,
	title = {Noise {Characteristic} and its {Removal} in digital {Radiographic} {System}},
	url = {https://www.ndt.net/article/wcndt00/papers/idn375/idn375.htm},
	urldate = {2024-08-04},
	file = {Noise Characteristic and its Removal in digital Radiographic System:C\:\\Users\\User\\Zotero\\storage\\I5DPASAR\\idn375.html:text/html},
}





%-------------------------------------------------------------------------------------------------------------
% DENOISING REFERENCES
%-------------------------------------------------------------------------------------------------------------

@article{kascenas_role_2023,
	title = {The role of noise in denoising models for anomaly detection in medical images},
	volume = {90},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841523002232},
	doi = {10.1016/j.media.2023.102963},
	abstract = {Pathological brain lesions exhibit diverse appearance in brain images, in terms of intensity, texture, shape, size, and location. Comprehensive sets of data and annotations are difficult to acquire. Therefore, unsupervised anomaly detection approaches have been proposed using only normal data for training, with the aim of detecting outlier anomalous voxels at test time. Denoising methods, for instance classical denoising autoencoders (DAEs) and more recently emerging diffusion models, are a promising approach, however naive application of pixelwise noise leads to poor anomaly detection performance. We show that optimization of the spatial resolution and magnitude of the noise improves the performance of different model training regimes, with similar noise parameter adjustments giving good performance for both DAEs and diffusion models. Visual inspection of the reconstructions suggests that the training noise influences the trade-off between the extent of the detail that is reconstructed and the extent of erasure of anomalies, both of which contribute to better anomaly detection performance. We validate our findings on two real-world datasets (tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain CT), showing good detection on diverse anomaly appearances. Overall, we find that a DAE trained with coarse noise is a fast and simple method that gives state-of-the-art accuracy. Diffusion models applied to anomaly detection are as yet in their infancy and provide a promising avenue for further research. Code for our DAE model and coarse noise is provided at: https://github.com/AntanasKascenas/DenoisingAE.},
	urldate = {2024-08-04},
	journal = {Medical Image Analysis},
	author = {Kascenas, Antanas and Sanchez, Pedro and Schrempf, Patrick and Wang, Chaoyang and Clackett, William and Mikhael, Shadia S. and Voisey, Jeremy P. and Goatman, Keith and Weir, Alexander and Pugeault, Nicolas and Tsaftaris, Sotirios A. and O’Neil, Alison Q.},
	month = dec,
	year = {2023},
	keywords = {Anomaly detection, Autoencoder, Denoising, Diffusion, Unsupervised learning},
	pages = {102963},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\5LSK5MM3\\S1361841523002232.html:text/html;Submitted Version:C\:\\Users\\User\\Zotero\\storage\\YMX6K99Y\\Kascenas et al. - 2023 - The role of noise in denoising models for anomaly .pdf:application/pdf},
}

@article{kumar_noise_2017,
	title = {Noise {Removal} and {Filtering} {Techniques} used in {Medical} {Images}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {09746471, 23208481},
	url = {http://www.computerscijournal.org/vol10no1/noise-removal-and-filtering-techniques-used-in-medical-images/},
	doi = {10.13005/ojcst/10.01.14},
	abstract = {Noise removal techniques have become an essential practice in medical imaging application for the study of anatomical structure and image processing of MRI medical images. To report these issues many de-noising algorithm has been developed like Weiner filter, Gaussian filter, median filter etc. In this research work is done with only three of the above filters which are already mentioned were successfully used in medical imaging. The most commonly affected noises in medical MRI image are Salt and Pepper, Speckle, Gaussian and Poisson noise. The medical images taken for comparison include MRI images, in gray scale and RGB. The performances of these algorithms are examined for various noise types which are salt-and-pepper, Poisson, speckle, blurred and Gaussian Noise. The evaluation of these algorithms is done by the measures of the image file size, histogram and clarity scale of the images. The median filter performs better for removing salt-and-pepper noise and Poisson Noise for images in gray scale, and Weiner filter performs better for removing Speckle and Gaussian Noise and Gaussian filter for the Blurred Noise as suggested in the experimental results.},
	language = {en},
	number = {1},
	urldate = {2024-08-04},
	journal = {Oriental journal of computer science and technology},
	author = {Kumar, Nalin and Nachamai, M},
	month = mar,
	year = {2017},
	pages = {103--113},
	file = {Kumar and Nachamai - 2017 - Noise Removal and Filtering Techniques used in Med.pdf:C\:\\Users\\User\\Zotero\\storage\\ZG6TF96L\\Kumar and Nachamai - 2017 - Noise Removal and Filtering Techniques used in Med.pdf:application/pdf},
}

@inproceedings{rodrigues_denoising_2008,
	title = {Denoising of medical images corrupted by {Poisson} noise},
	url = {https://ieeexplore.ieee.org/abstract/document/4712115},
	doi = {10.1109/ICIP.2008.4712115},
	abstract = {Medical images are often noisy owing to the physical mechanisms of the acquisition process. The great majority of the denoising algorithms assume additive white Gaussian noise. However, some of the most popular medical image modalities are degraded by some type of non-Gaussian noise. Among these types, we refer the Poisson noise, which is particularly suitable for modeling the counting processes associated to many imaging modalities such as PET, SPECT, and fluorescent confocal microscopy imaging. The aim of this work is to compare the effectiveness of several denoising algorithms in the presence of Poisson noise. We consider algorithms specifically designed for Poisson noise (wavelets, Platelets, and minimum descritpion length) and algorithms designed for Gaussian noise (edge preserving bilateral filtering, total variation, and non-local means). These algorithms are applied to piecewise smooth simulated and real data. Somehow unexpectedly, we conclude that total variation, designed for Gaussian noise, outperforms more elaborated state-of-the-art methods specifically designed for Poisson noise.},
	urldate = {2024-08-04},
	booktitle = {2008 15th {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Rodrigues, Isabel and Sanches, Joao and Bioucas-Dias, Jose},
	month = oct,
	year = {2008},
	note = {ISSN: 2381-8549},
	keywords = {Algorithm design and analysis, Biomedical imaging, Degradation, Gaussian noise, Denoising, Additive white noise, Bayesian, Filtering algorithms, Fluorescence, Microscopy, Multiplicative, Noise reduction, Non-local Means, Poisson, Positron emission tomography, Regularization, Total Variation, Wavelets},
	pages = {1756--1759},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\User\\Zotero\\storage\\E3KS3MD8\\4712115.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\2T77X279\\Rodrigues et al. - 2008 - Denoising of medical images corrupted by Poisson n.pdf:application/pdf},
}

@inproceedings{ravishankar_survey_2017,
	title = {A survey on noise reduction techniques in medical images},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/8203711},
	doi = {10.1109/ICECA.2017.8203711},
	abstract = {An image is a visual representation of the real world, represented in the form of two dimensional matrix consisting of x and y coordinates. Any image usually has a component of noise present in it. Image noise is an unwanted component due to random variation of brightness or colour information in images. Noise can be of various types including Gaussian noise and speckle noise. Rapid and unchecked cell growth in tissues of the lung is the primary cause of lung cancer. Computed Tomography (CT) scan images provide the detailed imaging of tumour growth inside the lungs and are widely used. However various noise types like those mentioned above can be encountered during a CT scan. Removal of these noises is critical for medical diagnoses and is done using filters. Filtering is a method by which we can enhance and denoise the image. This paper presents an overview of lung cancer and a survey on various noises and their removal techniques, including the usage of various filters.},
	urldate = {2024-08-04},
	booktitle = {2017 {International} conference of {Electronics}, {Communication} and {Aerospace} {Technology} ({ICECA})},
	author = {Ravishankar, Aditya and Anusha, S and Akshatha, H K and Raj, Anjali and Jahnavi, S and Madhura, J},
	month = apr,
	year = {2017},
	keywords = {Computed tomography, Digital filters, filters, Maximum likelihood detection, noise reduction, noise removal, Nonlinear filters, Optical filters, Wiener filters},
	pages = {385--389},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\User\\Zotero\\storage\\RUK96YAK\\8203711.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\MBWN49UM\\Ravishankar et al. - 2017 - A survey on noise reduction techniques in medical .pdf:application/pdf},
}

@article{shinde_study_2012,
	title = {Study of {Noise} {Detection} and {Noise} {Removal} {Techniques} in {Medical} {Images}},
	volume = {4},
	copyright = {Copyright Modern Education and Computer Science Press Mar 2012},
	issn = {20749074},
	url = {https://www.proquest.com/docview/1626672630/abstract/AB759FDEADFD425EPQ/1},
	abstract = {In this work we taken different medical images like MRI, Cancer, X-ray, and Brain and calculated standard derivations and mean of all these medical images. To finding salt \& pepper noise and then applied median filtering technique for removal of noise. After removing a noise by using median filtering techniques again standard derivations and mean are evaluated. This experimental analysis will improve the accuracy of MRI, Cancer, X-ray and Brain images for easy diagnosis. The results, which we have achieved, are more useful and they prove to be helpful for general medical practitioners to analyze the symptoms of the patients.},
	language = {English},
	number = {2},
	urldate = {2024-08-04},
	journal = {International Journal of Image, Graphics and Signal Processing},
	author = {Shinde, Bhausaheb and Mhaske, Dnyandeo and Dani, A. R.},
	month = mar,
	year = {2012},
	note = {Num Pages: 51-60
Place: Hong Kong, Hong Kong
Publisher: Modern Education and Computer Science Press},
	keywords = {Adaptive filter and Average filter, Brain, Cancer, Image, Magnetic resonance imaging, Median filter, MRI - Magnetic Resonance Imaging, ROI- Region Of Interest, X-ray},
	pages = {51--60},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\3Z2ETWF5\\Shinde et al. - 2012 - Study of Noise Detection and Noise Removal Techniq.pdf:application/pdf},
}

@article{thanh_review_2019,
	title = {A {Review} on {CT} and {X}-{Ray} {Images} {Denoising} {Methods}},
	volume = {43},
	copyright = {I assign to  Informatica ,  An International Journal of Computing and Informatics  ("Journal") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript ("Paper") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika},
	issn = {1854-3871},
	url = {https://www.informatica.si/index.php/informatica/article/view/2179},
	doi = {10.31449/inf.v43i2.2179},
	abstract = {In medical imaging systems, denoising is one of the important image processing tasks. Automatic noise removal will improve the quality of diagnosis and requires careful treatment of obtained imagery. Com-puted tomography (CT) and X-Ray imaging systems use the X radiation to capture images and they are usually corrupted by noise following a Poisson distribution. Due to the importance of Poisson noise re-moval in medical imaging, there are many state-of-the-art methods that have been studied in the image processing literature. These include methods that are based on total variation (TV) regularization, wave-lets, principal component analysis, machine learning etc. In this work, we will provide a review of the following important Poisson removal methods: the method based on the modified TV model, the adaptive TV method, the adaptive non-local total variation method, the method based on the higher-order natural image prior model, the Poisson reducing bilateral filter, the PURE-LET method, and the variance stabi-lizing transform-based methods. Our task focuses on methodology overview, accuracy, execution time and their advantage/disadvantage assessments. The goal of this paper is to provide an apt choice of denoising method that suits to CT and X-ray images. The integration of several high-quality denoising methods in image processing software for medical imaging systems will be always excellent option and help further image analysis for computer-aided diagnosis.},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Informatica},
	author = {Thanh, Dang and Surya, Prasath and Hieu, Le Minh},
	month = jun,
	year = {2019},
	note = {Number: 2},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\VNKSE5U4\\Thanh et al. - 2019 - A Review on CT and X-Ray Images Denoising Methods.pdf:application/pdf},
}

@article{juneja_denoising_2024,
	title = {Denoising techniques for cephalometric x-ray images: {A} comprehensive review},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Denoising techniques for cephalometric x-ray images},
	url = {https://doi.org/10.1007/s11042-023-17495-z},
	doi = {10.1007/s11042-023-17495-z},
	abstract = {Noising in X-ray imaging has been one of the biggest challenges that leads to insufficient and improper diagnosis. Despite the fact that X-rays are one of the most widespread and acceptable imaging techniques among the medical and scientific fraternity, still Gaussian and Poisson noise lead to a lot of image deterioration. Over the past few decades, several denoising techniques have been explored using traditional, hybrid and deep learning techniques which have been reported in this paper. Poisson noise was best removed by the application of bilateral filter with a maximum Peak Signal to Noise Ratio (PSNR) of 36.22 and for the removal of Gaussian noise, median filter proved to be unparalleled with a PSNR of 32.92 for the variance of 0.01, 31.4 for the variance of 0.04, 31.03 for the variance of 0.07, and 30.58 for the variance of 0.1 amongst the conventional filters. The Noise2Noise model employing the deep learning approach has given the best PSNR value of 34.38 amongst all the other alternatives for the images with gaussian noise. This paper serves as a comprehensive review for beginners working in this domain, that would aid them to select the best filter for the image pre-processing and noise removal.},
	language = {en},
	number = {17},
	urldate = {2024-08-08},
	journal = {Multimedia Tools and Applications},
	author = {Juneja, Mamta and Minhas, Janmejai Singh and Singla, Naveen and Kaur, Ravinder and Jindal, Prashant},
	month = may,
	year = {2024},
	keywords = {Deep learning, Gaussian noise, Denoising filters, Poisson noise, X-ray imaging},
	pages = {49953--49991},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\UDXZA5PF\\Juneja et al. - 2024 - Denoising techniques for cephalometric x-ray image.pdf:application/pdf},
}

@article{umadevi_improved_2011,
	title = {{IMPROVED} {HYBRID} {MODEL} {FOR} {DENOISING} {POISSON} {CORRUPTED} {X}-{RAY} {IMAGES}},
	volume = {3},
	abstract = {Medical practitioners are increasingly using digital images during disease diagnosis. Several state-of-the-art medical equipments are producing images of different organs, which are used during various stages of analysis. Examples of such devices include MRI, CT, ultrasound and X-Ray. Out of these, X-Ray is one the oldest and frequently used devices, as they are non-intrusive, painless and economical. The X-Ray images are normally affected by Poisson noise. The noise in the image has two disadvantages, the first being the degradation of the image quality and the second, more important, obscures important information required for accurate diagnosis. The main aim of any denoising algorithm is to remove noise while preserving important diagnostic data. This study combines two works that uses wavelets and Independent Component Analysis (ICA) to form a hybrid model that uses ICA technique coupled with Multiple Wavelet Denoising (MWD) Structure to remove noise. All the three works aim to remove Poisson noise from X-Ray images. Several experiments were conducted. The performance of the proposed system is analyzed in terms of Peak Signal to Noise Ratio and speed of denoising and a comparison is presented with the existing system.},
	language = {en},
	number = {7},
	author = {Umadevi, N and Geethalakshmi, Dr S N},
	year = {2011},
	file = {Umadevi and Geethalakshmi - 2011 - IMPROVED HYBRID MODEL FOR DENOISING POISSON CORRUP.pdf:C\:\\Users\\User\\Zotero\\storage\\J38P5EWZ\\Umadevi and Geethalakshmi - 2011 - IMPROVED HYBRID MODEL FOR DENOISING POISSON CORRUP.pdf:application/pdf},
}

@article{goreke_novel_2023,
	title = {A novel method based on {Wiener} filter for denoising {Poisson} noise from medical {X}-{Ray} images},
	volume = {79},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809422005080},
	doi = {10.1016/j.bspc.2022.104031},
	abstract = {Background and Objective
The Poisson noise is added to the image during the acquisition of medical X-Ray images. The distorted image due to this noise makes it difficult for physicians to diagnose the disease. Although there are various approaches for filtering Poisson noise, these approaches have disadvantages such as excessive smoothing of the image, distorting the texture information, reducing the image quality and high computational cost. In this study, a novel method that removes Poisson noise from medical X-Ray images is proposed by overcoming the above mentioned disadvantages.
Methods
In the proposed method, the Wiener filter is modified using the FIR filter embedded in the standard Wiener algorithm. The FIR filter design was carried out using the ASO optimization algorithm. Optimum local mean and optimum local variance values are calculated using the optimization matrix corresponding to the FIR filter coefficients and transferred to the standart Wiener filter layer as parameter inputs.
Results
The proposed method showed superior performance in synthetic images and medical X-Ray images in terms of PSNR, MSE, SSIM metrics and image quality metrics such as luminous intensity, Contrast index, Entropy and Sharpness. The time consumption of the proposed method is much less.
Conclusions
The clinical usage of the proposed method may help doctors to be able to diagnose the disease more accurately by interpreting the X-ray images. Besides, the proposed method can also have a positive effect on the CAD performance by using it at the pre-processing stage of CAD systems.},
	urldate = {2024-08-08},
	journal = {Biomedical Signal Processing and Control},
	author = {Göreke, Volkan},
	month = jan,
	year = {2023},
	keywords = {Poisson noise, Artificial intelligence, X-ray image filter},
	pages = {104031},
}

@article{kirti_poisson_2017,
	title = {Poisson noise reduction from {X}-ray images by region classification and response median filtering},
	volume = {42},
	issn = {0973-7677},
	url = {https://doi.org/10.1007/s12046-017-0654-4},
	doi = {10.1007/s12046-017-0654-4},
	abstract = {Medical imaging is perturbed with inherent noise such as speckle noise in ultrasound, Poisson noise in X-ray and Rician noise in MRI imaging. This paper focuses on X-ray image denoising problem. X-ray image quality could be improved by increasing dose value; however, this may result in cell death or similar kinds of issues. Therefore, image processing techniques are developed to minimise noise instead of increasing dose value for patient safety. In this paper, usage of modified Harris corner point detector to predict noisy pixels and responsive median filtering in spatial domain is proposed. Experimentation proved that the proposed work performs better than simple median filter and moving average (MA) filter. The results are very close to non-local means Poisson noise filter which is one of the current state-of-the-art methods. Benefits of the proposed work are simple noise prediction mechanism, good visual quality and less execution time.},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Sādhanā},
	author = {Kirti, Thakur and Jitendra, Kadam and Ashok, Sapkal},
	month = jun,
	year = {2017},
	keywords = {Poisson noise, modified Harris operator, region classification, response matrix, response median filtering},
	pages = {855--863},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\SGRISWVR\\Kirti et al. - 2017 - Poisson noise reduction from X-ray images by regio.pdf:application/pdf},
}

@article{khan_new_2016,
	title = {A new approach of weighted gradient filter for denoising of medical images in the presence of {Poisson} noise},
	volume = {23},
	issn = {1330-3651, 1848-6339},
	url = {https://hrcak.srce.hr/clanak/249928},
	doi = {10.17559/TV-20150620130712},
	abstract = {We propose a Weighted Gradient Filter for denoising of Poisson noise in medical images. In a predefined window, gradient of the centre pixel is averaged out. Gaussian Weighted filter is used on all calculated gradient values. Proposed method is appli...},
	language = {hr},
	number = {6},
	urldate = {2024-08-08},
	journal = {Tehnički vjesnik},
	author = {Khan, Khan Bahadar and Khaliq, Amir A. and Shahid, Muhammad and Shah, Jawad Ali},
	month = nov,
	year = {2016},
	note = {Publisher: Sveučilište u Slavonskom Brodu, Stojarski fakultet},
	pages = {1755--1762},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\AUEA3VJX\\Khan et al. - 2016 - A new approach of weighted gradient filter for den.pdf:application/pdf},
}

@article{wang_noise_2008,
	title = {Noise removal for medical {X}-ray images in wavelet domain},
	volume = {163},
	copyright = {Copyright © 2008 Wiley Periodicals, Inc.},
	issn = {1520-6416},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/eej.20486},
	doi = {10.1002/eej.20486},
	abstract = {Many important problems in engineering and science are well-modeled by Poisson noise, and the noise of medical X-ray images is Poisson noise. In this paper, we propose a method for noise removal for degraded medical X-ray images using improved preprocessing and an improved BayesShrink (IBS) method in the wavelet domain. First, we preprocess the medical X-ray image. Second, we apply the Daubechies (db) wavelet transform to medical X-ray images to acquire scaling and wavelet coefficients. Third, we apply the proposed IBS method to process wavelet coefficients. Finally, we compute the inverse wavelet transform for the threshold coefficients. Experimental results show that the proposed method always outperforms traditional methods. © 2008 Wiley Periodicals, Inc. Electr Eng Jpn, 163(3): 37– 46, 2008; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/eej.20486},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Electrical Engineering in Japan},
	author = {Wang, Ling and Lu, Jianming and Li, Yeqiu and Yahagi, Takashi and Okamoto, Takahide},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/eej.20486},
	keywords = {Poisson noise, BayesShrink method, medical X-ray image, wavelet},
	pages = {37--46},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\HM3VPSIQ\\Wang et al. - 2008 - Noise removal for medical X-ray images in wavelet .pdf:application/pdf},
}

@article{chandra_analysis_2020,
	title = {Analysis of quantum noise-reducing filters on chest {X}-ray images: {A} review},
	volume = {153},
	issn = {0263-2241},
	shorttitle = {Analysis of quantum noise-reducing filters on chest {X}-ray images},
	url = {https://www.sciencedirect.com/science/article/pii/S026322411931293X},
	doi = {10.1016/j.measurement.2019.107426},
	abstract = {Radiography is one of the important clinical adjuncts for preliminary disease investigation. The X-ray images are corrupted with inherent quantum noise affecting the performance of computer-aided diagnosis systems. This paper presents an extensive experimental review and impact of six benchmark filters for reducing noise and disease classification on chest X-ray images. The tradeoff between de-noising and texture preserving performance is investigated through classification performances using the state-of-the-art machine learning methods – Support Vector Machine and Artificial Neural Network. Moreover, the qualitative, subjective, and statistical evaluation is performed by using the image quality metrics, expert radiologist opinion, and statistical test, respectively. The experimental results confirm the significant improvement in classification performance using Guided filtered images. Furthermore, the results of qualitative measures and subjective analysis demonstrate that the guided filter and anisotropic diffusion filter both performed significantly better. Finally, a non-parametric statistical test is used to validate statistical significance of the obtained results.},
	urldate = {2024-08-08},
	journal = {Measurement},
	author = {Chandra, Tej Bahadur and Verma, Kesari},
	month = mar,
	year = {2020},
	keywords = {Poisson noise, De-noising, Image filtering, Noise filters, Quantum noise, X-ray images},
	pages = {107426},
}

@article{lahmiri_iterative_2017,
	title = {An iterative denoising system based on {Wiener} filtering with application to biomedical images},
	volume = {90},
	issn = {0030-3992},
	url = {https://www.sciencedirect.com/science/article/pii/S0030399216304480},
	doi = {10.1016/j.optlastec.2016.11.015},
	abstract = {Biomedical image denoising systems are important for accurate clinical diagnosis. The purpose of this study is to present a simple and effective iterative multistep image denoising system based on Wiener filtering (WF) where the denoised image from one stage is the input to the next stage. The denoising process stops when a particular condition measured by image energy is adaptively achieved. The proposed iterative system is tested on real clinical images and performance is measured by the well-known peak-signal-to-noise-ratio (PSNR) statistic. Experimental results showed that the proposed iterative system outperforms conventional image denoising algorithms; including wavelet packet (WP), fourth order partial differential equation (FOPDE), nonlocal Euclidean means (NLEM), first order local statistics (FOLS), and single Wiener filter used as baseline model. The experimental results demonstrate that the proposed approach can remove noise automatically and effectively while edges and texture characteristics are preserved.},
	urldate = {2024-08-08},
	journal = {Optics \& Laser Technology},
	author = {Lahmiri, Salim},
	month = may,
	year = {2017},
	keywords = {Denoising, Wiener filter, Biomedical images, Iterative system, Peak-signal-to-noise-ratio},
	pages = {128--132},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\AKCGXL2I\\S0030399216304480.html:text/html},
}

@inproceedings{hariharan_learning-based_2019,
	address = {Cham},
	title = {Learning-{Based} {X}-{Ray} {Image} {Denoising} {Utilizing} {Model}-{Based} {Image} {Simulations}},
	isbn = {978-3-030-32226-7},
	doi = {10.1007/978-3-030-32226-7_61},
	abstract = {X-ray guidance is an integral part of interventional procedures, but the exposure to ionizing radiation poses a non-negligible threat to patients and clinical staff. Unfortunately, a reduction in the X-ray dose results in a lower signal-to-noise ratio, which may impair the quality of X-ray images. To ensure an acceptable image quality while keeping the X-ray dose as low as possible, it is common practice to use denoising techniques. However, at very low dose levels, the application of conventional denoising techniques may lead to undesirable artifacts or oversmoothing. On the other hand, supervised learning techniques have outperformed conventional techniques in producing suitable results, provided aligned pairs of associated high- and low-dose X-ray images are available. Unfortunately, it is neither acceptable nor possible to acquire such image pairs during a clinical intervention. To enable the use of learning-based methods for the denoising of X-ray images, we propose a novel strategy that involves the use of model-based simulations of low-dose X-ray images during the training phase. We utilize a data-driven normalization step that increases the robustness of the proposed approach to varying amounts of signal-dependent noise associated with different X-ray image acquisition protocols. A quantitative and qualitative analysis based on clinical and phantom data shows that the proposed strategy outperforms well-established conventional X-ray image denoising methods. It also indicates that the proposed approach allows for a significant dose reduction without sacrificing important image information.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Hariharan, Sai Gokul and Kaethner, Christian and Strobel, Norbert and Kowarschik, Markus and Albarqouni, Shadi and Fahrig, Rebecca and Navab, Nassir},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	pages = {549--557},
}

@misc{noauthor_review_nodate,
	title = {A {Review} of {Denoising} {Medical} {Images} {Using} {Machine} {Learning} {Appro}...: {Ingenta} {Connect}},
	url = {https://www.ingentaconnect.com/content/ben/cmir/2018/00000014/00000005/art00003},
	urldate = {2024-08-08},
}

@article{dong_x-ray_2020,
	title = {X-ray image denoising based on wavelet transform and median filter},
	volume = {5},
	url = {https://sciendo.com/article/10.2478/amns.2020.2.00062},
	doi = {10.2478/amns.2020.2.00062},
	abstract = {This paper mainly proposed and researched based on wavelet transform, and then used the X-map denoising technique of value filter. In other words, the value image was filtered in the spatial domain, and the value filtering was used as the standard pulse (salt) noise, also used as in the wavelet domain. After the filtered image was decomposed by biorthogonal double wavelet transform, a wavelet coefficient matrix was generated, and a soft threshold quantisation process was performed on the wavelet coefficients to produce a new wavelet coefficient matrix. In the end, they used a new wavelet coefficient matrix for image reconstruction. The processing resulted that the denoising method proposed in this paper showed that the X image can be denoised, which not only reduced the X-picture-like noise but also preserved the X-picture-like details as much as possible. It also helped to enhance diagnostic accuracy and reduced the difference in reading.},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Applied Mathematics and Nonlinear Sciences},
	author = {Dong, Hanlei and Zhao, Liguo and Shu, Yunxing and Xiong, Neal N.},
	month = jul,
	year = {2020},
	pages = {435--442},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\4J6BSUH7\\Dong et al. - 2020 - X-ray image denoising based on wavelet transform a.pdf:application/pdf},
}

@article{mandic_denoising_2018,
	title = {Denoising of {X}-ray {Images} {Using} the {Adaptive} {Algorithm} {Based} on the {LPA}-{RICI} {Algorithm}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/4/2/34},
	doi = {10.3390/jimaging4020034},
	abstract = {Diagnostics and treatments of numerous diseases are highly dependent on the quality of captured medical images. However, noise (during both acquisition and transmission) is one of the main factors that reduce their quality. This paper proposes an adaptive image denoising algorithm applied to enhance X-ray images. The algorithm is based on the modification of the intersection of confidence intervals (ICI) rule, called relative intersection of confidence intervals (RICI) rule. For each image pixel apart, a 2D mask of adaptive size and shape is calculated and used in designing the 2D local polynomial approximation (LPA) filters for noise removal. One of the advantages of the proposed method is the fact that the estimation of the noise free pixel is performed independently for each image pixel and thus, the method is applicable for easy parallelization in order to improve its computational efficiency. The proposed method was compared to the Gaussian smoothing filters, total variation denoising and fixed size median filtering and was shown to outperform them both visually and in terms of the peak signal-to-noise ratio (PSNR) by up to 7.99 dB.},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Journal of Imaging},
	author = {Mandić, Ivica and Peić, Hajdi and Lerga, Jonatan and Štajduhar, Ivan},
	month = feb,
	year = {2018},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {medical imaging, adaptive filtering, image denoising, relative intersection of confidence interval (RICI) algorithm},
	pages = {34},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\XAB33X8D\\Mandić et al. - 2018 - Denoising of X-ray Images Using the Adaptive Algor.pdf:application/pdf},
}

@article{liu_noise_2013,
	title = {Noise reduction for low-dose {X}-ray {CT} based on fuzzy logical in stationary wavelet domain},
	volume = {124},
	issn = {0030-4026},
	url = {https://www.sciencedirect.com/science/article/pii/S0030402612008479},
	doi = {10.1016/j.ijleo.2012.10.044},
	abstract = {Apparent streak-like artifacts will present in reconstructed images due to excessive quantum noise in low-dose X-ray imaging process. Dealing with the noisy sinogram before a filtered back-projection (FBP) is a useful solution to solve this noise problem. In this paper, we proposed a novel noise restoration method combining wavelet and fuzzy logical technology for low-dose computed tomography (CT) sinogram. The method first utilizes stationary wavelet transform on the noisy sinogram, namely decomposes the sinogram to different resolution levels. And then, at each decomposed resolution level, a fuzzy shrinkage filter is applied to restore the noise-contaminated wavelet coefficients. Simulations were performed and indicated that the proposed method could significantly suppress noise and reduced streak-like artifacts in reconstructed images while at the same time maintaining the image sharpness.},
	number = {18},
	urldate = {2024-08-10},
	journal = {Optik - International Journal for Light and Electron Optics},
	author = {Liu, Yi and Gui, Zhiguo and Zhang, Quan},
	month = sep,
	year = {2013},
	keywords = {Noise reduction, FBP, Low-dose CT, Stationary wavelet transform},
	pages = {3348--3352},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\C6W48JV9\\S0030402612008479.html:text/html},
}


%-------------------------------------------------------------------------------------------------------------
% PCD Specific DENOISING REFERENCES
%-------------------------------------------------------------------------------------------------------------

@article{baffour_photon-counting_2023,
	title = {Photon-counting {Detector} {CT} with {Deep} {Learning} {Noise} {Reduction} to                     {Detect} {Multiple} {Myeloma}},
	volume = {306},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/full/10.1148/radiol.220311},
	doi = {10.1148/radiol.220311},
	abstract = {Background Photon-counting detector (PCD) CT and deep learning noise reduction may                             improve spatial resolution at lower radiation doses compared with                             energy-integrating detector (EID) CT.Purpose To demonstrate the diagnostic impact of improved spatial resolution in                             whole-body low-dose CT scans for viewing multiple myeloma by using PCD                             CT with deep learning denoising compared with conventional EID CT.Materials and Methods Between April and July 2021, adult participants who underwent a                             whole-body EID CT scan were prospectively enrolled and scanned with a                             PCD CT system in ultra-high-resolution mode at matched radiation dose (8                             mSv for an average adult) at an academic medical center. EID CT and PCD                             CT images were reconstructed with Br44 and Br64 kernels at 2-mm section                             thickness. PCD CT images were also reconstructed with Br44 and Br76                             kernels at 0.6-mm section thickness. The thinner PCD CT images were                             denoised by using a convolutional neural network. Image quality was                             objectively quantified in two phantoms and a randomly selected subset of                             participants (10 participants; median age, 63.5 years; five men). Two                             radiologists scored PCD CT images relative to EID CT by using a                             five-point Likert scale to detect findings reflecting multiple myeloma.                             The scoring for the matched reconstruction series was blinded to scanner                             type. Reader-averaged scores were tested with the null hypothesis of                             equivalent visualization between EID and PCD.Results Twenty-seven participants (median age, 68 years; IQR, 61–72 years;                             16 men) were included. The blinded assessment of 2-mm images                             demonstrated improvement in viewing lytic lesions, intramedullary                             lesions, fatty metamorphosis, and pathologic fractures for PCD CT versus                             EID CT (P {\textless} .05 for all comparisons). The 0.6-mm                             PCD CT images with convolutional neural network denoising also                             demonstrated improvement in viewing all four pathologic abnormalities                             and detected one or more lytic lesions in 21 of 27 participants compared                             with the 2-mm EID CT images (P {\textless} .001).Conclusion Ultra-high-resolution photon-counting detector CT improved the visibility                             of multiple myeloma lesions relative to energy-integrating detector                             CT. © RSNA, 2022 Online supplemental material is available for this                                     article.},
	number = {1},
	urldate = {2024-07-22},
	journal = {Radiology},
	author = {Baffour, Francis                             I. and Huber, Nathan                             R. and Ferrero, Andrea and Rajendran, Kishore and Glazebrook, Katrina                             N. and Larson, Nicholas                             B. and Kumar, Shaji and Cook, Joselle                             M. and Leng, Shuai and Shanblatt, Elisabeth                             R. and McCollough, Cynthia                             H. and Fletcher, Joel                             G.},
	month = jan,
	year = {2023},
	note = {Publisher: Radiological Society of North America},
	pages = {229--236},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\DG8QAUYZ\\Baffour et al. - 2023 - Photon-counting Detector CT with Deep Learning Noi.pdf:application/pdf},
}

@article{marcus_photon-counting_2024,
	title = {Photon-{Counting} {Detector} {CT} {With} {Denoising} for {Imaging} of the {Osseous} {Pelvis} at {Low} {Radiation} {Doses}: {A} {Phantom} {Study}},
	volume = {222},
	issn = {0361-803X},
	shorttitle = {Photon-{Counting} {Detector} {CT} {With} {Denoising} for {Imaging} of the {Osseous} {Pelvis} at {Low} {Radiation} {Doses}},
	url = {https://ajronline.org/doi/abs/10.2214/AJR.23.29765},
	doi = {10.2214/AJR.23.29765},
	abstract = {BACKGROUND. Photon-counting detector (PCD) CT may allow lower radiation doses than used for conventional energy-integrating detector (EID) CT, with preserved image quality.OBJECTIVE. The purpose of this study was to compare PCD CT and EID CT, reconstructed with and without a denoising tool, in terms of image quality of the osseous pelvis in a phantom, with attention to low radiation doses.METHODS. A pelvic phantom comprising human bones in acrylic material mimicking soft tissue underwent PCD CT and EID CT at various tube potentials and radiation doses ranging from 0.05 to 5.00 mGy. Additional denoised reconstructions were generated using a commercial tool. Noise was measured in the acrylic material. Two readers performed independent qualitative assessments that entailed determining the denoised EID CT reconstruction with the lowest acceptable dose and then comparing this reference reconstruction with PCD CT reconstructions without and with denoising, using subjective Likert scales.RESULTS. Noise was lower for PCD CT than for EID CT. For instance, at 0.05 mGy and 100 kV with tin filter, noise was 38.4 HU for PCD CT versus 48.8 HU for EID CT. Denoising further reduced noise; for example, for PCD CT at 100 kV with tin filter at 0.25 mGy, noise was 19.9 HU without denoising versus 9.7 HU with denoising. For both readers, lowest acceptable dose for EID CT was 0.10 mGy (total score, 11 of 15 for both readers). Both readers somewhat agreed that PCD CT without denoising at 0.10 mGy (reflecting reference reconstruction dose) was relatively better than the reference reconstruction in terms of osseous structures, artifacts, and image quality. Both readers also somewhat agreed that denoised PCD CT reconstructions at 0.10 mGy and 0.05 mGy (reflecting matched and lower doses, respectively, with respect to reference reconstruction dose) were relatively better than the reference reconstruction for the image quality measures.CONCLUSION. PCD CT showed better-quality images than EID CT when performed at the lowest acceptable radiation dose for EID CT. PCD CT with denoising yielded better-quality images at a dose lower than lowest acceptable dose for EID CT.CLINICAL IMPACT. PCD CT with denoising could facilitate lower radiation doses for pelvic imaging.},
	number = {1},
	urldate = {2024-07-22},
	journal = {American Journal of Roentgenology},
	author = {Marcus, Roy P. and Nagy, Daniel A. and Feuerriegel, Georg C. and Anhaus, Julian and Nanz, Daniel and Sutter, Reto},
	month = jan,
	year = {2024},
	note = {Publisher: American Roentgen Ray Society},
	pages = {e2329765},
}

@article{harrison_multichannel_2017,
	title = {A multichannel block-matching denoising algorithm for spectral photon-counting {CT} images},
	volume = {44},
	copyright = {Published 2017. This article is a U.S. Government work and is in the public domain in the USA.},
	issn = {2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mp.12225},
	doi = {10.1002/mp.12225},
	abstract = {Purpose We present a denoising algorithm designed for a whole-body prototype photon-counting computed tomography (PCCT) scanner with up to 4 energy thresholds and associated energy-binned images. Methods Spectral PCCT images can exhibit low signal to noise ratios (SNRs) due to the limited photon counts in each simultaneously-acquired energy bin. To help address this, our denoising method exploits the correlation and exact alignment between energy bins, adapting the highly-effective block-matching 3D (BM3D) denoising algorithm for PCCT. The original single-channel BM3D algorithm operates patch-by-patch. For each small patch in the image, a patch grouping action collects similar patches from the rest of the image, which are then collaboratively filtered together. The resulting performance hinges on accurate patch grouping. Our improved multi-channel version, called BM3D\_PCCT, incorporates two improvements. First, BM3D\_PCCT uses a more accurate shared patch grouping based on the image reconstructed from photons detected in all 4 energy bins. Second, BM3D\_PCCT performs a cross-channel decorrelation, adding a further dimension to the collaborative filtering process. These two improvements produce a more effective algorithm for PCCT denoising. Results Preliminary results compare BM3D\_PCCT against BM3D\_Naive, which denoises each energy bin independently. Experiments use a three-contrast PCCT image of a canine abdomen. Within five regions of interest, selected from paraspinal muscle, liver, and visceral fat, BM3D\_PCCT reduces the noise standard deviation by 65.0\%, compared to 40.4\% for BM3D\_Naive. Attenuation values of the contrast agents in calibration vials also cluster much tighter to their respective lines of best fit. Mean angular differences (in degrees) for the original, BM3D\_Naive, and BM3D\_PCCT images, respectively, were 15.61, 7.34, and 4.45 (iodine); 12.17, 7.17, and 4.39 (galodinium); and 12.86, 6.33, and 3.96 (bismuth). Conclusion We outline a multi-channel denoising algorithm tailored for spectral PCCT images, demonstrating improved performance over an independent, yet state-of-the-art, single-channel approach.},
	language = {en},
	number = {6},
	urldate = {2024-08-04},
	journal = {Medical Physics},
	author = {Harrison, Adam P. and Xu, Ziyue and Pourmorteza, Amir and Bluemke, David A. and Mollura, Daniel J.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12225},
	keywords = {block-matching 3D, collaborative filtering, denoising, multi-channel, photon-counting computed tomography},
	pages = {2447--2452},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\ITXGXU9U\\Harrison et al. - 2017 - A multichannel block-matching denoising algorithm .pdf:application/pdf;Snapshot:C\:\\Users\\User\\Zotero\\storage\\UJZ4WX53\\mp.html:text/html},
}

@article{nadkarni_deep_2023,
	title = {A {Deep} {Learning} {Approach} for {Rapid} and {Generalizable} {Denoising} of {Photon}-{Counting} {Micro}-{CT} {Images}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2379-139X},
	url = {https://www.mdpi.com/2379-139X/9/4/102},
	doi = {10.3390/tomography9040102},
	abstract = {Photon-counting CT (PCCT) is powerful for spectral imaging and material decomposition but produces noisy weighted filtered backprojection (wFBP) reconstructions. Although iterative reconstruction effectively denoises these images, it requires extensive computation time. To overcome this limitation, we propose a deep learning (DL) model, UnetU, which quickly estimates iterative reconstruction from wFBP. Utilizing a 2D U-net convolutional neural network (CNN) with a custom loss function and transformation of wFBP, UnetU promotes accurate material decomposition across various photon-counting detector (PCD) energy threshold settings. UnetU outperformed multi-energy non-local means (ME NLM) and a conventional denoising CNN called UnetwFBP in terms of root mean square error (RMSE) in test set reconstructions and their respective matrix inversion material decompositions. Qualitative results in reconstruction and material decomposition domains revealed that UnetU is the best approximation of iterative reconstruction. In reconstructions with varying undersampling factors from a high dose ex vivo scan, UnetU consistently gave higher structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) to the fully sampled iterative reconstruction than ME NLM and UnetwFBP. This research demonstrates UnetU’s potential as a fast (i.e., 15 times faster than iterative reconstruction) and generalizable approach for PCCT denoising, holding promise for advancing preclinical PCCT research.},
	language = {en},
	number = {4},
	urldate = {2024-08-04},
	journal = {Tomography},
	author = {Nadkarni, Rohan and Clark, Darin P. and Allphin, Alex J. and Badea, Cristian T.},
	month = aug,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, denoising, contrast agents, micro-CT, photon-counting CT, preclinical},
	pages = {1286--1302},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\YCBEQUQG\\Nadkarni et al. - 2023 - A Deep Learning Approach for Rapid and Generalizab.pdf:application/pdf},
}

@article{huber_dedicated_2022,
	title = {Dedicated convolutional neural network for noise reduction in ultra-high-resolution photon-counting detector computed tomography},
	volume = {67},
	issn = {0031-9155},
	url = {https://dx.doi.org/10.1088/1361-6560/ac8866},
	doi = {10.1088/1361-6560/ac8866},
	abstract = {Objective. To develop a convolutional neural network (CNN) noise reduction technique for ultra-high-resolution photon-counting detector computed tomography (UHR-PCD-CT) that can be efficiently implemented using only clinically available reconstructed images. The developed technique was demonstrated for skeletal survey, lung screening, and head angiography (CTA). Approach. There were 39 participants enrolled in this study, each received a UHR-PCD and an energy integrating detector (EID) CT scan. The developed CNN noise reduction technique uses image-based noise insertion and UHR-PCD-CT images to train a U-Net via supervised learning. For each application, 13 patient scans were reconstructed using filtered back projection (FBP) and iterative reconstruction (IR) and allocated into training, validation, and testing datasets (9:1:3). The subtraction of FBP and IR images resulted in approximately noise-only images. The 5-slice average of IR produced a thick reference image. The CNN training input consisted of thick reference images with reinsertion of spatially decoupled noise-only images. The training target consisted of the corresponding thick reference images without noise insertion. Performance was evaluated based on difference images, line profiles, noise measurements, nonlinear perturbation assessment, and radiologist visual assessment. UHR-PCD-CT images were compared with EID images (clinical standard). Main results. Up to 89\% noise reduction was achieved using the proposed CNN. Nonlinear perturbation assessment indicated reasonable retention of 1 mm radius and 1000 HU contrast signals ({\textgreater}80\% for skeletal survey and head CTA, {\textgreater}50\% for lung screening). A contour plot indicated reduced retention for small-radius and low contrast perturbations. Radiologists preferred CNN over IR for UHR-PCD-CT noise reduction. Additionally, UHR-PCD-CT with CNN was preferred over standard resolution EID-CT images. Significance. CT images reconstructed with very sharp kernels and/or thin sections suffer from increased image noise. Deep learning noise reduction can be used to offset noise level and increase utility of UHR-PCD-CT images.},
	language = {en},
	number = {17},
	urldate = {2024-08-04},
	journal = {Physics in Medicine \& Biology},
	author = {Huber, Nathan R. and Ferrero, Andrea and Rajendran, Kishore and Baffour, Francis and Glazebrook, Katrina N. and Diehn, Felix E. and Inoue, Akitoshi and Fletcher, Joel G. and Yu, Lifeng and Leng, Shuai and McCollough, Cynthia H.},
	month = sep,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {175014},
	file = {IOP Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\5NKPSSDX\\Huber et al. - 2022 - Dedicated convolutional neural network for noise r.pdf:application/pdf},
}

@article{chang_improved_2024,
	title = {Improved noise reduction in photon-counting detector {CT} using prior knowledge-aware iterative denoising neural network},
	volume = {11},
	issn = {2329-4302, 2329-4310},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-11/issue-S1/S12804/Improved-noise-reduction-in-photon-counting-detector-CT-using-prior/10.1117/1.JMI.11.S1.S12804.full},
	doi = {10.1117/1.JMI.11.S1.S12804},
	abstract = {PurposeWe aim to reduce image noise in high-resolution (HR) virtual monoenergetic images (VMIs) from photon-counting detector (PCD) CT scans by developing a prior knowledge-aware iterative denoising neural network (PKAID-Net) that efficiently exploits the unique noise characteristics of VMIs at different energy (keV) levels.ApproachPKAID-Net offers two major features: first, it utilizes a lower-noise VMI (e.g., 70 keV) as a prior input; second, it iteratively constructs a refined training dataset to improve the neural network’s denoising performance. In each iteration, the denoised image from the previous module serves as an updated target image, which is included in the dataset for the subsequent training iteration. Our study includes 10 patient coronary CT angiography exams acquired on a clinical dual-source PCD-CT (NAEOTOM Alpha, Siemens Healthineers). The HR VMIs were reconstructed at 50, 70, and 100 keV, using a sharp vascular kernel (Bv68) and thin (0.6 mm) slice thickness (0.3 mm increment). PKAID-Net’s performance was evaluated in terms of image noise, spatial detail preservation, and quantitative accuracy.ResultsPKAID-Net achieved a noise reduction of 96\% compared to filtered back projection and 65\% relative to iterative reconstruction, all while preserving spatial and spectral fidelity and maintaining a natural noise texture. The iterative refinement of PCD-CT data during the training process substantially enhanced the robustness of deep learning-based denoising compared to the original method, which resulted in some spatial detail loss.ConclusionsThe PKAID-Net provides substantial noise reduction while maintaining spatial and spectral fidelity of the HR VMIs from PCD-CT.},
	number = {S1},
	urldate = {2024-08-04},
	journal = {Journal of Medical Imaging},
	author = {Chang, Shaojie and Jr, Jeffrey F. Marsh and Koons, Emily K. and Gong, Hao and McCollough, Cynthia H. and Leng, Shuai},
	month = may,
	year = {2024},
	note = {Publisher: SPIE},
	pages = {S12804},
}

@article{lee_performance_2019,
	title = {Performance evaluation of total variation ({TV}) denoising technique for dual-energy contrast-enhanced digital mammography ({CEDM}) with photon counting detector ({PCD}): {Monte} {Carlo} simulation study},
	volume = {156},
	issn = {0969-806X},
	shorttitle = {Performance evaluation of total variation ({TV}) denoising technique for dual-energy contrast-enhanced digital mammography ({CEDM}) with photon counting detector ({PCD})},
	url = {https://www.sciencedirect.com/science/article/pii/S0969806X18303645},
	doi = {10.1016/j.radphyschem.2018.10.028},
	abstract = {The dual-energy contrast-enhanced digital mammography (CEDM) system based on a photon counting detector (PCD) is very useful providing functional information for breast cancer detection. In particular, this system can be used to solve the spectral overlap and high radiation dose problems. However, imaging noise is a big problem because of the degradation image performance and cancer detection ratio in the CEDM system. To address this problem, a total variation (TV)-based denoising technique approach has recently been studied. Thus, the aim of this study was to evaluate and confirm the image performance of our TV-based denoising technique with dual-energy CEDM with a PCD. For this purpose, we simulated a dual-energy CEDM with a PCD and breast phantom in Monte Carlo simulation using the Geant4 Application for Tomographic Emission (GATE) that is an essential open source program. We also designed a TV-based denoising technique based on the L1-norm estimation included correction and iteration steps for acquiring high edge preservation in X-ray images. To evaluate the image performance, we used evaluation parameters with a contrast-to-noise ratio (CNR) and coefficient of variation (COV) as a function of the absorbed dose levels (2.18, 1.53, 1.09, and 0.66 mGy). According to the results, the average of all iodine thicknesses and absorbed dose conditions for the CNR using our proposed TV-based denoising technique was 1.71, 1.39, and 1.13 times higher than that acquired for the noisy image, median filter and Wiener filter, respectively. We also acquired excellent COV results for the dual-energy CEDM with a PCD system (2.53 times higher than that of the noisy image). In conclusion, the results of this study suggested that a TV-based denoising technique can be achieved with an improved image performance and the effect and feasibility of the TV-based denoising technique for dual-energy CEDM with a PCD can be investigated.},
	urldate = {2024-08-08},
	journal = {Radiation Physics and Chemistry},
	author = {Lee, Seungwan and Lee, Youngjin},
	month = mar,
	year = {2019},
	keywords = {Contrast-enhanced digital mammography (CEDM), Dual-energy imaging method, Evaluation of image performance, Monte Carlo simulation, Photon counting detector (PCD), Total variation (TV)-based denoising technique},
	pages = {94--100},
}

@article{lee_x-ray_2018,
	title = {X-ray image denoising with fast non-local means ({FNLM}) approach using acceleration function in {CdTe} semiconductor photon counting detector ({PCD}): {Monte} {Carlo} simulation study},
	volume = {172},
	issn = {0030-4026},
	shorttitle = {X-ray image denoising with fast non-local means ({FNLM}) approach using acceleration function in {CdTe} semiconductor photon counting detector ({PCD})},
	url = {https://www.sciencedirect.com/science/article/pii/S0030402618310234},
	doi = {10.1016/j.ijleo.2018.07.051},
	abstract = {The noise reduction algorithms have been widely used in the field of medical X-ray imaging system using semiconductor photon counting detector (PCD). Among these algorithms, fast non-local means (FNLM) method is based on use of the encoded information and considering the local region in the image. The aim of this study was to design FNLM noise reduction algorithm using acceleration function with Euclidean distance, which is feasible to cope with practical problems, and to confirm its application feasibility in CdTe PCD. We simulated X-ray imaging system with PCD using Monte Carlo simulation tool (Geant4 Application for Tomographic Emission (GATE)) version 6 and applied conventional and proposed noise reduction algorithms in acquired water phantom image. We conclude that the results of the image performances associated with noise characteristics demonstrated the superiority of the proposed FNLM noise reduction algorithm in CdTe PCD imaging system.},
	urldate = {2024-08-08},
	journal = {Optik},
	author = {Lee, Youngjin},
	month = nov,
	year = {2018},
	keywords = {Monte Carlo simulation, CdTe photon counting detector, Non-local means algorithm using acceleration function, Quantitative evaluation of image performance, X-ray denoising},
	pages = {456--461},
}

@inproceedings{vanmeter_quantification_2022,
	title = {Quantification of coronary calcification using high-resolution photon-counting-detector {CT} and an image domain denoising algorithm},
	volume = {12031},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12031/120311R/Quantification-of-coronary-calcification-using-high-resolution-photon-counting-detector/10.1117/12.2612999.full},
	doi = {10.1117/12.2612999},
	abstract = {Coronary artery calcification is an important indicator of coronary disease. Accurate volume quantification of coronary calcification using computed tomography (CT) is challenging due to calcium blooming. In this study, ex-vivo coronary specimens were scanned on an investigational photon-counting detector (PCD) CT scanner and the estimated coronary calcification volume were compared with a conventional energy-integrating detector (EID) CT. An image-based denoising algorithm was applied to the PCD-CT images to achieve similar noise levels as EID-CT. Calcifications were segmented to estimate the volume, with micro-CT images of the same calcifications serving as reference. PCD-CT images showed reduced calcium blooming artifacts.},
	urldate = {2024-08-08},
	booktitle = {Medical {Imaging} 2022: {Physics} of {Medical} {Imaging}},
	publisher = {SPIE},
	author = {VanMeter, Patrick D. and Jr, Jeffery Marsh and Rajendran, Kishore and Leng, Shuai and McCollough, Cynthia},
	month = apr,
	year = {2022},
	pages = {433--438},
	file = {Accepted Version:C\:\\Users\\User\\Zotero\\storage\\9E4KWLW3\\VanMeter et al. - 2022 - Quantification of coronary calcification using hig.pdf:application/pdf},
}

@inproceedings{ren_simulation_2023,
	title = {A simulation study on photon-counting denoising based on subspace decomposition},
	volume = {12748},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12748/127480E/A-simulation-study-on-photon-counting-denoising-based-on-subspace/10.1117/12.2689447.full},
	doi = {10.1117/12.2689447},
	abstract = {Photon counting detector (PCD) is a hot topic at present. Compared with traditional energy integral detector, it has the potential of high spatial resolution, high sensitivity and low dose, which can effectively promote medical imaging diagnosis. However, when PCD is counting X-ray photons, the photon number of each energy bin is relatively small. Additionally, charge-sharing response and pulse superposition effect will also affect the photon count rate, resulting in serious noise and affecting the imaging quality. In this paper, a photon-counting denoising algorithm based on subspace decomposition is proposed. According to the similarity between the data of different bins and the self-similarity of the data, this paper constructs sparse representation by subspace decomposition method and uses block matching algorithm to suppress noise. In simulation experiments, we carried out spectral computed tomography imaging experiments with the three-dimensional phantom of a digital mice based on PCD, and denoised the data by different algorithms. The quantitative results show that our method improves peak signal-to-noise ratio by 2.21dB compared with block-matching and 3D filtering when photon flux is 4×10$^{\textrm{3}}$ , which verifies the potential of the proposed algorithm in medical imaging.},
	urldate = {2024-08-08},
	booktitle = {5th {International} {Conference} on {Information} {Science}, {Electrical}, and {Automation} {Engineering} ({ISEAE} 2023)},
	publisher = {SPIE},
	author = {Ren, Junru and Cai, Ailong and Liang, Ningning and Wang, Yizhong and Zhang, Xinrui and Li, Lei and Yan, Bin},
	month = aug,
	year = {2023},
	pages = {80--86},
}

@misc{li_x-ray_2020,
	title = {X-ray {Photon}-{Counting} {Data} {Correction} through {Deep} {Learning}},
	url = {http://arxiv.org/abs/2007.03119},
	doi = {10.48550/arXiv.2007.03119},
	abstract = {X-ray photon-counting detectors (PCDs) are drawing an increasing attention in recent years due to their low noise and energy discrimination capabilities. The energy/spectral dimension associated with PCDs potentially brings great benefits such as for material decomposition, beam hardening and metal artifact reduction, as well as low-dose CT imaging. However, X-ray PCDs are currently limited by several technical issues, particularly charge splitting (including charge sharing and K-shell fluorescence re-absorption or escaping) and pulse pile-up effects which distort the energy spectrum and compromise the data quality. Correction of raw PCD measurements with hardware improvement and analytic modeling is rather expensive and complicated. Hence, here we proposed a deep neural network based PCD data correction approach which directly maps imperfect data to the ideal data in the supervised learning mode. In this work, we first establish a complete simulation model incorporating the charge splitting and pulse pile-up effects. The simulated PCD data and the ground truth counterparts are then fed to a specially designed deep adversarial network for PCD data correction. Next, the trained network is used to correct separately generated PCD data. The test results demonstrate that the trained network successfully recovers the ideal spectrum from the distorted measurement within \${\textbackslash}pm6{\textbackslash}\%\$ relative error. Significant data and image fidelity improvements are clearly observed in both projection and reconstruction domains.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Li, Mengzhou and Rundle, David S. and Wang, Ge},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03119 [physics]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Physics - Medical Physics},
	file = {arXiv Fulltext PDF:C\:\\Users\\User\\Zotero\\storage\\Y7GFR37K\\Li et al. - 2020 - X-ray Photon-Counting Data Correction through Deep.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\User\\Zotero\\storage\\L8RAVH6C\\2007.html:text/html},
}

@inproceedings{sun_research_2024,
	address = {New York, NY, USA},
	series = {{ICAICE} '23},
	title = {Research on noise reduction technology of {PCD} spectral imaging based on {Transformer}},
	isbn = {9798400708831},
	url = {https://dl.acm.org/doi/10.1145/3652628.3652675},
	doi = {10.1145/3652628.3652675},
	abstract = {Spectral computed tomography (CT) utilizing photon counting detector (PCD) has advanced quickly in recent years, and its utilization in clinics has been gradually increasing. Compared with traditional CT, PCD-CT has the advantages of multi substances quantification, and can significantly reduce image noise and scanning dose while maintaining image quality. However, due to the limitation of hardware, PCD is affected by photon starving, charge sharing effect and pulse superposition effect, leading to statistical fluctuation noise in spectral CT reconstruction. Nowadays, deep learning has become the state-of-the-art method in noise suppressing of PCD-CT. In this work, we introduced the attention mechanism based on Transformer into CNN backbone and proposed a new reconstruction U-Net. This architecture can effectively explore both global and local features of noisy maps. The qualitative and quantitative experiments indicated that our method can effectively improve the performance in noise reduction and spectral CT reconstruction.},
	urldate = {2024-08-08},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Wenyao and Cui, Bokun and Lan, Zhejie and Li, Longji and Tang, Xuefu and Zhang, Xinrui and Ren, Junru and Liang, Ningning and Lei, Li and Bin, Yan},
	month = may,
	year = {2024},
	pages = {286--290},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\M9MVNBJ6\\Sun et al. - 2024 - Research on noise reduction technology of PCD spec.pdf:application/pdf},
}

@article{shim_feasibility_2018,
	title = {Feasibility of newly designed fast non local means ({FNLM})-based noise reduction filter for {X}-ray imaging: {A} simulation study},
	volume = {160},
	issn = {0030-4026},
	shorttitle = {Feasibility of newly designed fast non local means ({FNLM})-based noise reduction filter for {X}-ray imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S0030402618301207},
	doi = {10.1016/j.ijleo.2018.01.101},
	abstract = {In the diagnostic radiology field, reducing the radiation dose for patient lead to increased noise in image. Since increases of noise decrease the diagnosis rate, to reduce the noise is necessary. In this study quantitatively evaluates the four widely used and newly verified filters which remove noise in image: median, Wiener, total variation, and fast non local means (FNLM). For that purpose, X-ray and computed tomography (CT) images are acquired using MATLAB simulation with 3D voxelized phantom. To evaluate image performance, normalized noise power spectrum (NNPS), contrast to noise ratio (CNR) and coefficient of variation (COV) were used. As a result, we can efficiently remove noise in X-ray image when FNLM filter was used compared with frequently used filters. In conclusion, our results demonstrated that our proposed FNLM filter shows superior denoising performance, which is expected to enhance the detection of diseases in clinical images with low dose.},
	urldate = {2024-08-08},
	journal = {Optik},
	author = {Shim, Jina and Yoon, Myonggeun and Lee, Youngjin},
	month = may,
	year = {2018},
	keywords = {Median filter, Fast non local means (FNLM) filter, Image performance evaluation, Simulation study, Total variation (TV) filter, Wiener filter},
	pages = {124--130},
}

@article{kipele_poisson_2023,
	title = {Poisson {Noise} {Reduction} with {Nonlocal}-{PCA} {Hybrid} {Model} in {Medical} {X}-ray {Images}},
	doi = {10.18178/joig.11.2.178-184},
	abstract = {The presence of Poisson noise in medical X-ray images leads to degradation of the image quality. The obscured information is required for accurate diagnosis. During X-ray image acquisition process, weak light results into limited number of available photons, which leads into the Poisson noise commonly known as X-ray noise. Currently, the available X-ray noise removal methods have not yet obtained satisfying total denoising results to remove noise from the medical X-ray images. The available techniques tend to show good performance when the image model corresponds to the algorithm’s assumptions used but in general, the denoising algorithms fail to do complete denoise. X-ray image quality could be improved by increasing the X-ray dose value (beyond the maximum medically permissible dose) but the process could be lethal to patients’ health since higher X-ray energy may kill cells due to the effects of higher dose values. In this study, the hybrid model that combines the Poisson Principal Component Analysis (Poisson PCA) with the nonlocal (NL) means denoising algorithm is developed to reduce noise in images. This hybrid model for X-ray noise removal and the contrast enhancement improves the quality of X-ray images and can, thus, be used for medical diagnosis. The performance of the proposed hybrid model was observed by using the standard data and was compared with the standard Poisson algorithms.},
	journal = {Journal of Image and Graphics},
	author = {Kipele, Daniel and Greyson, Kenedy},
	month = jun,
	year = {2023},
	pages = {178--184},
}

@article{lee_impact_2022,
	title = {The impact of improved non-local means denoising algorithm on photon-counting {X}-ray images using various {Al} additive filtrations},
	volume = {1027},
	issn = {0168-9002},
	url = {https://www.sciencedirect.com/science/article/pii/S0168900221010846},
	doi = {10.1016/j.nima.2021.166244},
	abstract = {The filters used in X-ray systems affect the image quality and radiation dose. Although using filters has advantages in terms of image quality and dose, image noise increases because the amount of X-rays in the energy range of the entire area is reduced. Thus, a newly improved non-local means (INLM) denoising algorithm was modeled and applied to X-ray images based on the thickness of the aluminum (Al) filter, which is most commonly used in X-ray imaging systems, to prove its potential use. For X-ray image acquisition, a high-performance cadmium telluride material-based detector and a tube containing Al filters (1-, 3-, and 5-mm thickness) were designed. The proposed INLM denoising algorithm was modeled by including the improved weights for the gradient information for each pixel in the conventional NLM-based equation. When the proposed INLM denoising algorithm was applied to the acquired photon-counting X-ray images, results showed superior performance for both noise characteristics and no-reference-based image evaluation. In particular, we confirmed that when the INLM was applied to X-ray images using 5 mm Al thickness, noise characteristics and no-reference-based evaluation results were improved by approximately 1.36 times and 1.06 times, respectively, compared to the conventional NLM. The study proved that choosing the INLM in photon-counting X-ray images using 5 mm-thick Al filtration is vital for the success of image processing applications.},
	urldate = {2024-08-08},
	journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	author = {Lee, Seungwan and Lee, Youngjin},
	month = mar,
	year = {2022},
	keywords = {Al filtration, Denoising algorithm, Improved non-local means (INLM) approach, Photon-counting X-ray imaging, Quantitative evaluation of image quality},
	pages = {166244},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\43M54CEC\\S0168900221010846.html:text/html},
}


@article{thakur_poisson_2016,
	series = {Proceedings of {International} {Conference} on {Communication}, {Computing} and {Virtualization} ({ICCCV}) 2016},
	title = {Poisson {Noise} {Reducing} {Bilateral} {Filter}},
	volume = {79},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050916002180},
	doi = {10.1016/j.procs.2016.03.087},
	abstract = {Noise removal is a classical problem but not yet solve completely. So researchers are attracted to this problem again and again. Poisson noise is signal dependent noise and to remove this kind of noise, additive noise removal techniques are not helpful. Existing state of art methods such as non-local mean filter, bilateral filter, BM3D algorithms works better for reducing additive noise. Non local mean filter and BM3D algorithms are further modified for Poisson noise reduction but they require more execution time to remove the noise. In this paper we proposed a spatial domain filter by modifying bilateral filter framework. Advantages of proposed filter are non-iterative nature, simplicity and edge preserving ability. It gives image quality comparable to current state of art method such as non-local mean filtering. Also, time required for this modified bilateral filter is very less than other techniques.},
	urldate = {2024-09-07},
	journal = {Procedia Computer Science},
	author = {Thakur, Kirti V. and Damodare, Omkar H. and Sapkal, Ashok M.},
	month = jan,
	year = {2016},
	keywords = {Bilateral Filter., Non-local mean filter, Poisson Noise},
	pages = {861--865},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\R8QWYLBW\\S1877050916002180.html:text/html},
}



@article{rudin_nonlinear_1992,
	title = {Nonlinear total variation based noise removal algorithms},
	volume = {60},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/016727899290242F},
	doi = {10.1016/0167-2789(92)90242-F},
	abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lanrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t → ∞ the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
	number = {1},
	urldate = {2024-09-07},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Rudin, Leonid I. and Osher, Stanley and Fatemi, Emad},
	month = nov,
	year = {1992},
	pages = {259--268},
	file = {ScienceDirect Snapshot:C\:\\Users\\User\\Zotero\\storage\\UG6VIGU7\\016727899290242F.html:text/html},
}


@article{prasath_quantum_2017,
	title = {Quantum {Noise} {Removal} in {X}-{Ray} {Images} with {Adaptive} {Total} {Variation} {Regularization}},
	volume = {28},
	issn = {0868-4952, 1822-8844},
	url = {https://informatica.vu.lt/journal/INFORMATICA/article/859},
	doi = {10.15388/Informatica.2017.141},
	abstract = {Medical X-ray images are prevalent and are the least expensive diagnostic imaging method available widely. The handling of film processing and digitization introduces noise in X-ray images and suppressing such noise is an important step in medical image analysis. In this work, we use an adaptive total variation regularization method for removing quantum noise from X-ray images. By utilizing an edge indicator measure along with the well-known edge preserving total variation regularization, we obtain noise removal without losing salient features. Experimental results on different X-ray images indicate the promise of our approach. Synthetic examples are given to compare the performance of our scheme with traditional total variation and anisotropic diffusion methods from the literature. Overall, our proposed approach obtains better results in terms of visual appearance as well as with respect to different error metrics and structural similarity.},
	language = {en},
	number = {3},
	urldate = {2024-09-07},
	journal = {Informatica},
	author = {Prasath, V. B. Surya},
	month = jan,
	year = {2017},
	note = {Publisher: Vilnius University Institute of Data Science and Digital Technologies},
	pages = {505--515},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\6XDNJH4P\\Prasath - 2017 - Quantum Noise Removal in X-Ray Images with Adaptiv.pdf:application/pdf},
}


@article{le_variational_2007,
	title = {A {Variational} {Approach} to {Reconstructing} {Images} {Corrupted} by {Poisson} {Noise}},
	volume = {27},
	issn = {1573-7683},
	url = {https://doi.org/10.1007/s10851-007-0652-y},
	doi = {10.1007/s10851-007-0652-y},
	abstract = {We propose a new variational model to denoise an image corrupted by Poisson noise. Like the ROF model described in [1] and [2], the new model uses total-variation regularization, which preserves edges. Unlike the ROF model, our model uses a data-fidelity term that is suitable for Poisson noise. The result is that the strength of the regularization is signal dependent, precisely like Poisson noise. Noise of varying scales will be removed by our model, while preserving low-contrast features in regions of low intensity.},
	language = {en},
	number = {3},
	urldate = {2024-09-07},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Le, Triet and Chartrand, Rick and Asaki, Thomas J.},
	month = apr,
	year = {2007},
	keywords = {image denoising, image processing, image reconstruction, Poisson noise, radiography, total variation},
	pages = {257--263},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\VFAD75Q9\\Le et al. - 2007 - A Variational Approach to Reconstructing Images Co.pdf:application/pdf},
}


@article{liu_poisson_2017,
	title = {Poisson noise removal based on nonlocal total variation with {Euler}’s elastica pre-processing},
	volume = {22},
	issn = {1995-8188},
	url = {https://doi.org/10.1007/s12204-017-1878-5},
	doi = {10.1007/s12204-017-1878-5},
	abstract = {An enhancement-based Poisson denoising method for photon-limited images is presented. The noisy image is firstly pre-processed for enhancing incomplete object information, and then it is denoised while preserving the restored structural details. A variational regularization model based on Euler’s elastica (EE) is proposed for image enhancement pre-processing. A nonlocal total variation (NLTV) regularization model is then employed in the second stage of image denoising. The above two optimization problems are solved by the alternating direction method of multipliers (ADMM). For Poissonian images with low image peak values, experiments demonstrate the validity and efficiency of the proposed method for both restoring geometric structure and removing noise.},
	language = {en},
	number = {5},
	urldate = {2024-09-07},
	journal = {Journal of Shanghai Jiaotong University (Science)},
	author = {Liu, Hongyi and Zhang, Zhengrong and Xiao, Liang and Wei, Zhihui},
	month = oct,
	year = {2017},
	keywords = {A, Euler’s elastica (EE), nonlocal total variation (NLTV), Poisson noise, TP 391, variation regularization},
	pages = {609--614},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\YNGR6FD8\\Liu et al. - 2017 - Poisson noise removal based on nonlocal total vari.pdf:application/pdf},
}



@inproceedings{dey_deconvolution_2004,
	title = {A deconvolution method for confocal microscopy with total variation regularization},
	url = {https://ieeexplore.ieee.org/abstract/document/1398765},
	doi = {10.1109/ISBI.2004.1398765},
	abstract = {Confocal laser scanning microscopy is a powerful and increasingly popular technique for 3D imaging of biological specimens. However the acquired images are degraded by blur from out-of-focus light and Poisson noise due to photon-limited detection. Several deconvolution methods have been proposed to reduce these degradations, including the Richardson-Lucy algorithm, which computes a maximum likelihood estimation adapted to Poisson statistics. However this method tends to amplify noise if used without regularizing constraint. Here, we propose to combine the Richardson-Lucy algorithm with a regularizing constraint based on total variation, whose smoothing avoids oscillations while preserving edges. We show on simulated images that this constraint improves the deconvolution result both visually and using quantitative measures.},
	urldate = {2024-09-07},
	booktitle = {2004 2nd {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {Nano} to {Macro} ({IEEE} {Cat} {No}. {04EX821})},
	author = {Dey, N. and Blanc-Feraud, L. and Zimmer, C. and Kam, Z. and Olivo-Marin, J.-C. and Zerubia, J.},
	month = apr,
	year = {2004},
	keywords = {Biology computing, Computational modeling, Deconvolution, Degradation, Laser noise, Maximum likelihood estimation, Microscopy, Power lasers, Smoothing methods, Statistics},
	pages = {1223--1226 Vol. 2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\User\\Zotero\\storage\\HXDXQCW8\\1398765.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\EAD3PHZM\\Dey et al. - 2004 - A deconvolution method for confocal microscopy wit.pdf:application/pdf},
}


@ARTICLE{1542113,
  author={Mahmoudi, M. and Sapiro, G.},
  journal={IEEE Signal Processing Letters}, 
  title={Fast image and video denoising via nonlocal means of similar neighborhoods}, 
  year={2005},
  volume={12},
  number={12},
  pages={839-842},
  keywords={Noise reduction;Pixel;Image denoising;Filters;Computational complexity;Image processing;Image restoration;Image sampling;Costs;Acceleration;Computational complexity;contexts;image and video denoising;nonlocal neighborhood filters},
  doi={10.1109/LSP.2005.859509}}


@inproceedings{coupe_fast_2006,
	address = {Berlin, Heidelberg},
	title = {Fast {Non} {Local} {Means} {Denoising} for {3D} {MR} {Images}},
	isbn = {978-3-540-44728-3},
	doi = {10.1007/11866763_5},
	abstract = {One critical issue in the context of image restoration is the problem of noise removal while keeping the integrity of relevant image information. Denoising is a crucial step to increase image conspicuity and to improve the performances of all the processings needed for quantitative imaging analysis. The method proposed in this paper is based on an optimized version of the Non Local (NL) Means algorithm. This approach uses the natural redundancy of information in image to remove the noise. Tests were carried out on synthetic datasets and on real 3T MR images. The results show that the NL-means approach outperforms other classical denoising methods, such as Anisotropic Diffusion Filter and Total Variation.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2006},
	publisher = {Springer},
	author = {Coupé, Pierrick and Yger, Pierre and Barillot, Christian},
	editor = {Larsen, Rasmus and Nielsen, Mads and Sporring, Jon},
	year = {2006},
	keywords = {Denoising Algorithm, Denoising Method, Ground Truth, Noise Removal, Search Volume},
	pages = {33--40},
	file = {Full Text PDF:C\:\\Users\\User\\Zotero\\storage\\8Q9WJALU\\Coupé et al. - 2006 - Fast Non Local Means Denoising for 3D MR Images.pdf:application/pdf},
}

@INPROCEEDINGS{5653394,
  author={Deledalle, Charles-Alban and Tupin, Florence and Denis, Loïc},
  booktitle={2010 IEEE International Conference on Image Processing}, 
  title={Poisson NL means: Unsupervised non local means for Poisson noise}, 
  year={2010},
  volume={},
  number={},
  pages={801-804},
  keywords={Noise measurement;Noise reduction;Pixel;Optimized production technology;Probabilistic logic;Gaussian noise;Non local means;Poisson noise;mean square error;SURE;PURE;Newton's method},
  doi={10.1109/ICIP.2010.5653394}}


@misc{noauthor_wiener_nodate,
	title = {The {Wiener} filter},
	url = {https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/VELDHUIZEN/node15.html},
	urldate = {2024-09-07},
	file = {The Wiener filter:C\:\\Users\\User\\Zotero\\storage\\9ILCGG7W\\node15.html:text/html},
}

@article{LUISIER2010415,
title = {Fast interscale wavelet denoising of Poisson-corrupted images},
journal = {Signal Processing},
volume = {90},
number = {2},
pages = {415-427},
year = {2010},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2009.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0165168409003016},
author = {Florian Luisier and Cédric Vonesch and Thierry Blu and Michael Unser},
keywords = {Poisson, Interscale, Denoising, Wavelets, Risk estimation, Linear expansion of thresholds, Fluorescence microscopy},
abstract = {We present a fast algorithm for image restoration in the presence of Poisson noise. Our approach is based on (1) the minimization of an unbiased estimate of the MSE for Poisson noise, (2) a linear parametrization of the denoising process and (3) the preservation of Poisson statistics across scales within the Haar DWT. The minimization of the MSE estimate is performed independently in each wavelet subband, but this is equivalent to a global image-domain MSE minimization, thanks to the orthogonality of Haar wavelets. This is an important difference with standard Poisson noise-removal methods, in particular those that rely on a non-linear preprocessing of the data to stabilize the variance. Our non-redundant interscale wavelet thresholding outperforms standard variance-stabilizing schemes, even when the latter are applied in a translation-invariant setting (cycle-spinning). It also achieves a quality similar to a state-of-the-art multiscale method that was specially developed for Poisson data. Considering that the computational complexity of our method is orders of magnitude lower, it is a very competitive alternative. The proposed approach is particularly promising in the context of low signal intensities and/or large data sets. This is illustrated experimentally with the denoising of low-count fluorescence micrographs of a biological sample.}
}

@ARTICLE{4531116,
  author={Zhang, Bo and Fadili, Jalal M. and Starck, Jean-Luc},
  journal={IEEE Transactions on Image Processing}, 
  title={Wavelets, Ridgelets, and Curvelets for Poisson Noise Removal}, 
  year={2008},
  volume={17},
  number={7},
  pages={1093-1108},
  keywords={Noise reduction;Wavelet domain;Testing;Discrete wavelet transforms;Filter bank;Wiener filter;Bayesian methods;Maximum likelihood estimation;Gaussian noise;Gaussian processes;Curvelets;filtered Poisson process;multiscale variance stabilizing transform;Poisson intensity estimation;ridgelets;wavelets},
  doi={10.1109/TIP.2008.924386}}

@ARTICLE{761328,
  author={Timmermann, K.E. and Nowak, R.D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Multiscale modeling and estimation of Poisson processes with application to photon-limited imaging}, 
  year={1999},
  volume={45},
  number={3},
  pages={846-862},
  keywords={Bayesian methods;Nuclear medicine;Photonics;Wavelet transforms;Image analysis;Wavelet analysis;Biomedical imaging;Astronomy;Two dimensional displays;Application software},
  doi={10.1109/18.761328}}

@INPROCEEDINGS{6568145,
  author={Du, Lingyan and Wen, Yuqiao and Ma, Jiang},
  booktitle={2013 Fourth International Conference on Intelligent Control and Information Processing (ICICIP)}, 
  title={Dual tree complex wavelet transform and Bayesian estimation based denoising of poission-corrupted X-ray images}, 
  year={2013},
  volume={},
  number={},
  pages={598-603},
  keywords={Noise;Noise reduction;Bayes methods;Discrete wavelet transforms;X-ray imaging;Poisson;dual tree complex wavelet transform;Bayesian estimation;X-ray image denoising},
  doi={10.1109/ICICIP.2013.6568145}}

@INPROCEEDINGS{1273218,
  author={Gupta, S. and Kaur, L. and Chauhan, R.C. and Saxena, S.C.},
  booktitle={TENCON 2003. Conference on Convergent Technologies for Asia-Pacific Region}, 
  title={A wavelet based statistical approach for speckle reduction in medical ultrasound images}, 
  year={2003},
  volume={2},
  number={},
  pages={534-537 Vol.2},
  keywords={Speckle;Biomedical imaging;Ultrasonic imaging;Medical diagnostic imaging;Wiener filter;Wavelet coefficients;Image processing;Bayesian methods;Noise reduction;Parameter estimation},
  doi={10.1109/TENCON.2003.1273218}}

@ARTICLE{6212354,
  author={Makitalo, Markku and Foi, Alessandro},
  journal={IEEE Transactions on Image Processing}, 
  title={Optimal Inversion of the Generalized Anscombe Transformation for Poisson-Gaussian Noise}, 
  year={2013},
  volume={22},
  number={1},
  pages={91-103},
  keywords={Noise reduction;Approximation methods;Gaussian noise;Standards;Accuracy;Photonics;Denoising;photon-limited imaging;Poisson-Gaussian noise;variance stabilization},
  doi={10.1109/TIP.2012.2202675}}

@article{salmon2014poisson,
  title={Poisson noise reduction with non-local PCA},
  author={Salmon, Joseph and Harmany, Zachary and Deledalle, Charles-Alban and Willett, Rebecca},
  journal={Journal of mathematical imaging and vision},
  volume={48},
  pages={279--294},
  year={2014},
  publisher={Springer}
}

@article{doi:10.1137/16M1072930,
author = {Feng, Wensen and Qiao, Hong and Chen, Yunjin},
title = {Poisson Noise Reduction with Higher-Order Natural Image Prior Model},
journal = {SIAM Journal on Imaging Sciences},
volume = {9},
number = {3},
pages = {1502-1524},
year = {2016},
doi = {10.1137/16M1072930},

URL = { 
    
        https://doi.org/10.1137/16M1072930
    
    

},
eprint = { 
    
        https://doi.org/10.1137/16M1072930
    
    

}
,
    abstract = { Poisson denoising is an essential issue for various imaging applications, such as night vision, medical imaging, and microscopy. State-of-the-art approaches are clearly dominated by patch-based non-local methods in recent years. In this paper, we aim to propose a local Poisson denoising model with both structural simplicity and good performance. To this end, we consider a variational modeling to integrate the so-called fields of experts (FoE) image prior, that has proven an effective higher-order Markov random fields model for many classic image restoration problems. We exploit several feasible variational variants for this task. We start with a direct modeling in the original image domain by taking into account the Poisson noise statistics, which performs generally well for the cases of high signal-to-noise ratio (SNR). However, this strategy encounters problem in cases of low SNR. Then we turn to an alternative modeling strategy by using the Anscombe transform and Gaussian statistics derived data term. We retrain the FoE prior model directly in the transform domain. With the newly trained FoE model, we end up with a local variational model providing strongly competitive results against state-of-the-art nonlocal approaches, meanwhile bearing the property of simple structure. Furthermore, our proposed model comes along with an additional advantage, that the inference is very efficient as it is well suited for parallel computation on GPUs. For images of size \$512 \times 512\$, our GPU implementation takes less than 1 second to produce state-of-the-art Poisson denoising performance. }
}


@article{jisha2014poisson,
  title={Poisson noise removal in biomedical images using non-linear techniques},
  author={Jisha, JU and Sureshkumar, V},
  journal={International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering},
  volume={3},
  pages={131--136},
  year={2014}
}


@ARTICLE{4271520,
  author={Dabov, Kostadin and Foi, Alessandro and Katkovnik, Vladimir and Egiazarian, Karen},
  journal={IEEE Transactions on Image Processing}, 
  title={Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering}, 
  year={2007},
  volume={16},
  number={8},
  pages={2080-2095},
  keywords={Image denoising;Collaboration;Filtering;Noise reduction;Signal processing algorithms;Signal processing;Energy resolution;Spatial resolution;Signal resolution;Discrete cosine transforms;Adaptive grouping;block matching;image denoising;sparsity;3-D transform shrinkage},
  doi={10.1109/TIP.2007.901238}}

@ARTICLE{6319316,
  author={He, Kaiming and Sun, Jian and Tang, Xiaoou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Guided Image Filtering}, 
  year={2013},
  volume={35},
  number={6},
  pages={1397-1409},
  keywords={Image edge detection;Kernel;Smoothing methods;Joints;Histograms;Laplace equations;Jacobian matrices;Edge-preserving filtering;bilateral filter;linear time filtering},
  doi={10.1109/TPAMI.2012.213}}

@ARTICLE{7559744,
  author={Treece, Graham},
  journal={IEEE Transactions on Image Processing}, 
  title={The Bitonic Filter: Linear Filtering in an Edge-Preserving Morphological Framework}, 
  year={2016},
  volume={25},
  number={11},
  pages={5199-5211},
  keywords={Robustness;Image edge detection;Noise reduction;Two dimensional displays;Noise level;Context;Frequency-domain analysis;Gaussian filter;median filter;morphology;noise reduction;edge preservation},
  doi={10.1109/TIP.2016.2605302}}


@InProceedings{pmlr-v80-lehtinen18a,
  title = 	 {{N}oise2{N}oise: Learning Image Restoration without Clean Data},
  author =       {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2965--2974},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/lehtinen18a/lehtinen18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/lehtinen18a.html},
  abstract = 	 {We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.}
}


@inproceedings{wang2016accelerating,
  title={Accelerating magnetic resonance imaging via deep learning},
  author={Wang, Shanshan and Su, Zhenghang and Ying, Leslie and Peng, Xi and Zhu, Shun and Liang, Feng and Feng, Dagan and Liang, Dong},
  booktitle={2016 IEEE 13th international symposium on biomedical imaging (ISBI)},
  pages={514--517},
  year={2016},
  organization={IEEE}
}


@INPROCEEDINGS{7950457,
  author={Lee, Dongwook and Yoo, Jaejun and Ye, Jong Chul},
  booktitle={2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)}, 
  title={Deep residual learning for compressed sensing MRI}, 
  year={2017},
  volume={},
  number={},
  pages={15-18},
  keywords={Image reconstruction;Complexity theory;Magnetic resonance imaging;Topology;Manifolds;Machine learning;Compressed sensing MRI;deep learning;residual learning;CNN},
  doi={10.1109/ISBI.2017.7950457}}


@INPROCEEDINGS{8954066,
  author={Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Noise2Void - Learning Denoising From Single Noisy Images}, 
  year={2019},
  volume={},
  number={},
  pages={2124-2132},
  keywords={Training;Photography;Deep learning;Noise reduction;Fluorescence;Pattern recognition;Noise measurement;Medical;Biological and Cell Microscopy;Deep Learning;Low-level Vision;Statistical Learning},
  doi={10.1109/CVPR.2019.00223}}

@ARTICLE{7839189,
  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
  journal={IEEE Transactions on Image Processing}, 
  title={Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising}, 
  year={2017},
  volume={26},
  number={7},
  pages={3142-3155},
  keywords={Noise reduction;Image denoising;Training;Computational modeling;Noise level;Neural networks;Transform coding;Image denoising;convolutional neural networks;residual learning;batch normalization},
  doi={10.1109/TIP.2017.2662206}}


@article{ducros2017regularization,
  title={Regularization of nonlinear decomposition of spectral x-ray projection images},
  author={Ducros, Nicolas and Abascal, Juan Felipe Perez-Juste and Sixou, Bruno and Rit, Simon and Peyrin, Fran{\c{c}}oise},
  journal={Medical physics},
  volume={44},
  number={9},
  pages={e174--e187},
  year={2017},
  publisher={Wiley Online Library}
}

@article{zhang2020multi,
  title={Multi-energy CT reconstruction using tensor nonlocal similarity and spatial sparsity regularization},
  author={Zhang, Wenkun and Liang, Ningning and Wang, Zhe and Cai, Ailong and Wang, Linyuan and Tang, Chao and Zheng, Zhizhong and Li, Lei and Yan, Bin and Hu, Guoen},
  journal={Quantitative Imaging in Medicine and Surgery},
  volume={10},
  number={10},
  pages={1940},
  year={2020},
  publisher={AME Publications}
}